{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e7698a3-3b8c-4c43-a9e0-683081bbbe51",
   "metadata": {},
   "source": [
    "Here, we describe how to train PreMet on new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "533b6ca9-522a-4aa9-973b-b19450f583c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import anndata as ann\n",
    "import scanpy as sc\n",
    "import os, sys\n",
    "from scipy.stats import pearsonr as pr\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score as f1\n",
    "from sklearn.metrics import precision_recall_curve as prc\n",
    "from sklearn.metrics import silhouette_score as sil\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import precision_score, recall_score, average_precision_score\n",
    "import random\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d84829f-04c1-48c1-8c04-23cd91214708",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7e9fb9-599f-48e3-b941-c647fed41f9e",
   "metadata": {},
   "source": [
    "# Generate random data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725c5ded-e52e-4e30-a7f5-2e1b05da8fd6",
   "metadata": {},
   "source": [
    "Here, we're using some random data as an example; noted that if sample number is small, it's better to limit the gene number to under 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b9b4fb2-c082-4bda-bbe2-4c12738d2be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tumor_type = 3 # number of tumor types\n",
    "num_mets_site = 2 # number of metastasis sites\n",
    "num_genes = 1000  # Number of genes\n",
    "num_samples = 200  # Number of samples\n",
    "num_clusters = num_tumor_type*num_mets_site  \n",
    "cluster_spread = 1.5  # Variability between clusters\n",
    "gene_expression_min = 3  # Average minimum expression level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "10abd7ff-9f89-4542-a54a-77ae2a03faf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate cluster centers (mean expression levels for each cluster)\n",
    "cluster_centers = np.random.normal(loc=gene_expression_min, scale=cluster_spread, size=(num_clusters, num_genes))\n",
    "\n",
    "# Generate labels for each sample (which cluster they belong to)\n",
    "sample_labels = np.random.choice(num_clusters, num_samples)\n",
    "\n",
    "# Create an empty matrix to hold the RNA-seq data\n",
    "simulated_data = np.zeros((num_samples, num_genes))\n",
    "\n",
    "# Generate RNA-seq like data with noise\n",
    "for i in range(num_samples):\n",
    "    # Get the cluster center for this sample\n",
    "    cluster_center = cluster_centers[sample_labels[i], :]\n",
    "    \n",
    "    # Add noise to simulate RNA-seq counts\n",
    "    simulated_data[i, :] = np.random.negative_binomial(n=10, p=np.clip(1 - cluster_center / (cluster_center + 10), 0.01, 0.99))\n",
    "\n",
    "# Convert to pandas DataFrame for easy manipulation\n",
    "simulated_df = pd.DataFrame(simulated_data, columns=[f'Gene_{i+1}' for i in range(num_genes)], index=[f'Sample_{i+1}' for i in range(num_samples)])\n",
    "simulated_df['Cluster'] = sample_labels\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(simulated_df.drop('Cluster', axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9f0f9176-5f87-4a3b-9a68-b465e4fd74e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 3, 3, 3, 2, 2, 2, 2, 1, 2, 3, 2, 1, 1, 2, 3, 3, 3, 3, 2, 2, 2,\n",
       "       1, 1, 2, 1, 2, 1, 3, 3, 2, 2, 2, 3, 1, 3, 2, 3, 2, 1, 3, 3, 1, 2,\n",
       "       3, 1, 3, 3, 1, 2, 2, 1, 1, 3, 2, 3, 3, 2, 1, 1, 3, 1, 1, 2, 1, 1,\n",
       "       3, 1, 1, 3, 3, 3, 1, 3, 1, 2, 1, 3, 1, 1, 1, 3, 1, 3, 2, 3, 1, 3,\n",
       "       2, 1, 1, 1, 3, 1, 3, 2, 3, 2, 1, 2, 3, 2, 1, 1, 1, 3, 3, 2, 1, 1,\n",
       "       2, 2, 3, 3, 3, 2, 2, 1, 1, 1, 2, 3, 2, 3, 2, 3, 1, 1, 3, 2, 3, 1,\n",
       "       2, 3, 2, 2, 2, 3, 1, 1, 2, 1, 3, 1, 3, 1, 2, 2, 1, 3, 2, 3, 3, 1,\n",
       "       1, 2, 2, 1, 2, 3, 1, 2, 1, 1, 1, 2, 3, 1, 3, 1, 3, 1, 1, 2, 1, 2,\n",
       "       1, 2, 2, 3, 3, 3, 1, 2, 1, 2, 1, 1, 1, 1, 3, 2, 3, 1, 1, 2, 2, 3,\n",
       "       1, 2])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate random labels for primary tumor types\n",
    "rand_tumor_label = np.random.randint(0, num_tumor_type, size=num_samples)+1\n",
    "rand_tumor_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1e88afa7-e017-4b6a-ac4e-a6f1cf6786f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1, 2, 1, 1, 1, 2, 1, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 1, 1, 2, 2,\n",
       "       2, 2, 1, 2, 1, 1, 1, 2, 2, 2, 1, 2, 1, 1, 1, 1, 2, 2, 1, 2, 1, 2,\n",
       "       2, 2, 2, 1, 2, 2, 2, 2, 2, 1, 1, 1, 2, 1, 1, 1, 1, 2, 2, 2, 2, 2,\n",
       "       1, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 1,\n",
       "       2, 2, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 2, 1, 1, 1, 2, 2, 1, 1, 2, 1,\n",
       "       2, 2, 2, 1, 1, 2, 2, 2, 1, 1, 2, 2, 2, 2, 1, 2, 2, 1, 2, 2, 2, 1,\n",
       "       2, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 2, 2, 1,\n",
       "       1, 2, 1, 1, 2, 2, 1, 2, 2, 1, 2, 1, 2, 2, 1, 1, 1, 2, 2, 1, 2, 1,\n",
       "       2, 1, 1, 1, 2, 2, 2, 1, 2, 2, 1, 2, 1, 2, 1, 2, 1, 1, 1, 2, 2, 1,\n",
       "       1, 1])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate random labels for primary tumor types\n",
    "\n",
    "rand_metas_site_label = np.random.randint(0, num_mets_site, size=num_samples)+1\n",
    "rand_metas_site_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c92b4cbc-0ae4-4a28-a212-7b0376ed03b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load it into anndata\n",
    "adata = ann.AnnData(scaled_data)\n",
    "adata.obs_names = simulated_df.index\n",
    "adata.var_names = simulated_df.drop('Cluster', axis=1).columns\n",
    "adata.obs['metas_site'] = rand_metas_site_label.astype(str)\n",
    "adata.obs['tumor'] = rand_tumor_label.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4e647943-47bf-40b0-a434-5dbe3943cd64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wang3712/.local/lib/python3.8/site-packages/scanpy/plotting/_tools/scatterplots.py:392: UserWarning: No data for colormapping provided via 'c'. Parameters 'cmap' will be ignored\n",
      "  cax = scatter(\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlkAAAGvCAYAAABo28DeAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAABV/klEQVR4nO3dd5gV1eHG8e/MLdsru8vSiwhSRBBRsaLBgoJdAUFFNMYWYzRG04hJTNSY/KKxJgawYAPsXVSKiqCCIIiASK8LbLlb7u4tM78/VhfWbbdu4/08j0+4Z845c67EnXfPnDlj2LZtIyIiIiIxZbb0AERERETaI4UsERERkThQyBIRERGJA4UsERERkThQyBIRERGJA4UsERERkThQyBIRERGJA4UsERERkThQyBIRERGJA4UsERERkThQyBI5CC1atIg777yT4uLilh6KiEi7pZAlchBatGgRf/rTnxSyRETiSCFLRNqUQCCAz+dr6WGIiDRJIUvkIHPnnXdy2223AdCrVy8Mw8AwDDZt2oRhGDzxxBN12hiGwZ133lmrD8MwWLduHZMmTSIjI4Pc3Fz+8Ic/YNs2W7du5dxzzyU9PZ38/Hz++c9/1umzoKCAq666io4dO5KYmMgRRxzBk08+WavOD2P6xz/+wf33388hhxxCQkICq1evjum/ExGReHC29ABEpHldcMEFrFu3jueee45//etf5OTkAFBeXh52X+PGjaN///7cc889vPnmm9x1111kZ2fzn//8h1NPPZV7772XZ555hl/96lcMHz6ck046CQCv18vIkSNZv349N954I7169WL27NlMnjyZ4uJifvGLX9Q6z4wZM6isrOSaa64hISGB7Ozs6P9FiIjEmy0iB5377rvPBuyNGzfWlG3cuNEG7BkzZtSpD9h//OMfaz7/8Y9/tAH7mmuuqSkLBAJ2165dbcMw7HvuuaemvKioyE5KSrKvuOKKmrL777/fBuyZM2fWlPl8PnvEiBF2amqq7fF4ao0pPT3dLigoiP6Li4g0I90uFJGIXX311TV/djgcHHXUUdi2zVVXXVVTnpmZSb9+/diwYUNN2VtvvUV+fj4TJkyoKXO5XNx0002UlZWxYMGCWue58MILyc3NjeM3ERGJPYUsEYlY9+7da33OyMggMTGx5hbkgeVFRUU1nzdv3syhhx6Kadb+EdS/f/+a4wfq1atXLIctItIsFLJEBKhe3F6fYDDYYBuHwxFSGYBt25ENDEhKSoq4rYhIS1HIEjkI1ReosrKyAOrsnfXjWaVY6NGjB99++y2WZdUqX7NmTc1xEZG2TiFL5CCUkpIC1A5U6enp5OTksHDhwlp1H3nkkZif/6yzzmLXrl288MILNWWBQIAHH3yQ1NRUTj755JifU0SkuWkLB5GD0LBhwwD43e9+x/jx43G5XIwdO5arr76ae+65h6uvvpqjjjqKhQsXsm7dupif/5prruE///kPkydPZunSpfTs2ZM5c+bwySefcP/995OWlhbzc4qINDeFLJGD0PDhw/nLX/7CY489xjvvvINlWWzcuJGpU6eyZ88e5syZw6xZsxg9ejRvv/02eXl5MT1/UlIS8+fP54477uDJJ5/E4/HQr18/ZsyYweTJk2N6LhGRlmLY0axGFREREZF6aU2WiIiISBwoZImIiIjEgUKWiIiISBwoZImIiIjEgUKWiIiISBwoZImIiIjEQYvsk2VZFjt27CAtLa3B96WJiIhI62LbNqWlpXTu3LnOC96lrhYJWTt27KBbt24tcWoRERGJ0tatW+natWtLD6PVa5GQ9cMrM7Zu3Up6enpLDEFERETC5PF46Natm159FaIWCVk/3CJMT09XyBIREWljtNQnNLqhKiIiIhIHClkiIiIicaCQJSIiIhIHClkiIiIicaCQJSIiIhIHLfJ0oYiIiDTMtm0W7VjEi9++yLbSbVQGK0l1pTKs4zAu6XsJ3dK112RboJAlIiLSCliWzYJ1e3hs6SzW+1/GZxTUqbNy70qeWPU0vZ0XYHhOZFthAK8vSEqCk8O7ZDBpRA9G9s3VFguthEKWiIhIcwj6YfWrsPkTqPSAMxGyemIPmcATq/zM+GQTuxwvk5AzDxrISL6i4fj2nsaKQDpQUVNe7gvywZoCPlhTQI8OyfzurP6cPjC/eb6XNEghS0REJJ4qS+CTf8Oyp6C87uyUNe9vdA4eScfkbhR1XNpgN1UFp+Pbd2qTp9u8r4JrZy7lz+cOYtKxPaIaukRHC99FRETipWQ7TD8TPvpHvQELwIHFGY4veLbqZc4pLau3Tm7B8JAC1g8sG6a+uor3vt4V0bAlNhSyRERE4sFbDDMvgILVIVV3AXftLeTlbTu4vqiYvECA9KDFQzuK2LPvzLBPb9lwz9trsG077LYSG7pdKCIiEg8f/gX2rAmriQH08QfoU+zhp8UeKg2DD/3HUEpKREPYsLecj9fv5cRDcyNqL9HRTJaIiEisVZXCihei6sIJpNo2JzlWMsjYEHE/Mz9ZH9U4JHIKWSIiIrG24nnwlcakq0yjnCfcf6ebsTui9l+vXQuv3lgd/KRZKWSJiIjE2rdz6y1eavUhYId/6c0xPNzujGxmrMxOhC+fhumjoXxvRH1IZBSyREREYq2yuOaPPtvB04FRnFl1Dx8Ej8RpWBF1ebr5ObkUN1nvx1KMyuo/7F4Jz40Hf2VE55fwKWSJiIjEmln9XFmJnczl/jv4Q2AKa+2uXOr8MOIu3UaQ8Y7w2+9L3sMz6anVH7Z9Dl89H/EYJDwKWSIiIrGW3oVK28VVvttYbA0EoIuxl65GdLfrhptrw25jZH3GPR2y+WdWZnXB59OiGoOETiFLREQk1o4Yx0OB8/jC7ldTlIY36m7TjPD6MFyFOFKrg9kTmenMTE+DXV/B1s+iHos0TSFLREQkxnx5R/C89ZNaZZW4o+630g6vj4TcdzGM/ZuRPpKZgdcwYNsXUY9FmqaQJSIiEkvFW3n7sdvZa6fXKt5tZ+ENMyT92GY7L+S67ty3cWWsqFVW6jB5OyUZqjxRjUNCo5AlIiISK94imHkB7xZ3rXOogkTeCB4bVfcuAqRR0Wgdw1FGYqfZJOQsqPf4rPRUcCVHNQ4JjUKWiIhIrCy4D/auo5DUeg8/FTw9qu4vdH7M4oQbuMs5jQHGJpwEqg8YfhxJm0js/Dwph96NK3Npg32sdbshp29U45DQ6N2FIiIiseD3wvJnAHBQ/0uZV9q9mRc8glMcK+o9HooUo4pJzg+Y5PyAjS4nY7t0xQhj762AYeDrfXIMVohJUzSTJSIiEgurXqzZhLSxTUN/7v85q6yeMTnl54mJYQUsANO22ektiMn5pXEKWSIiIrGwYf8aqHMcixqsVkYy432/54Pg0KhP+UJ6/bclG2MABRUKWc1BIUtERCQWDniVzkhzBd2MhoNMGclc5b+NsVV/oSLCJw6XJSSwzh1+26Bh4Lf8EZ1TwqOQJSIiEgvOhJo/mobNZMc7TTZZaR/CTf6fE6x/CVeDyg2Dv3XICneENTLcGRG3ldApZLUR24u9fL2jhG93l1JSod9ARERancwetT5OcbzDGPPTJpu9bw3j1/5r8duOkE5Tahj8vGMuaxMimwHLS8qjX3a/pitK1PR0YStWWunnxaXbeGbJFr4tKKspd5gGo/rncdmxPTm+TwcMwwipv+3FXoorfDhNk7y0BLJS9GyJiEjMDJkInz5U89Ew4F+uR0gM+JgTPLnRpi9aJ1HmT+CG5GkMDpTVWycALExO4sGsDNZHcJvwBxf2vRCnqct/czBs2w5zkjJ6Ho+HjIwMSkpKSE9Pb7rBQej1FTu448WvKPcFG603sHMal43oydsrd7FudykVviApbgeDumQw8dgeDOueycvLd/DM4s2s2VVa0840YGS/PC47tgcn983FNEMLaiIi0ojpo2FL3UXvi4IDeCp4Ou9bRxL40fxGT2MXEx3vc4ljAV5nJX/rkMkxlVX09gVIsi3KTJOvE9zMTktlt3N/W4dtc25pGZ8kJ9Uq/7HsYJBi08QyDJyGk3cvepe85NB3jj+Qrt/hUchqhV74fAt3vLSSWPzNGAZN9tO3Yyr/u3w43TtoB2ARkah8OxeeuRga2Cdrl53FZ9ZheOxkXPjpbuzhWPMbDrwhUWoY/C8znVnpaZSZdVf1mLbNCd5KphR7GFZVRRCYn5zEC+mpfON2U2aaJFsWh/r8XFxahmHb3N4xF4CpI6Zycd+LI/56un6HRyGrlfl8UyET/ruYgBX5X0s6ZRxprGeBPRg7xGV3OaluZl97HL1yUiI+r4iIAJ/8G+b+oclq/8rKoMI0OausnNxgENOGYofJvORk5qSlstdZvUbLZdmc5PWSbNl0CQQ4r6yMLoHG73IcaEp+Hp8lJuEsPoflN98V8dcCXb/DpZuyrcwj89ZHHLC6Gnv4ueNlDjG2c6n/9yEHLIC9ZT4u+s+HzP3lyWQn6z8cEZGIHX8TJKTC27dD0NdgtSKHg5fTUnk+Pa3R7vymwQhvJeNK61+r1Zh1LhefBgfj23ISHV2Dwm4v0dHTha3I1sIKFqzbE1Hbw40NvOz+A+Oc85kRPBMfrrD72FdqctoTv2HV3lURjUFERL531BT45Wr4yVRI71xvleQwfqH+Z3Ymq93h/VwvIoFrK+/Au3UKwYo+dMlKCqu9RE8hqxWZvXQblg39jc1McszlOsdrXOl4m5Hmckwafm1CL2MnT7rvIdfwUGBn8J41POIxFBUczpR3p7Bq7yr8QYuich+V/tCnpUVE5HupuXDirXDz13DOw2DWDkl9fQ3Pcv2Y1zS5Jj+PZQkJTVcGdtrZXFp1Jxv8/WvKLh7WLeTzSWzodmFrEfSTt/EVXnTPYpj5LQDFpkmRw8SwoTKYyeu+U3k2eCpF1L6d92fnDLKN6mnkl4In4o/ir9Wq7EbRzuO44OEl+Lyba8o7ZyQy/ujujD+6G3lpiRH3LyJy0DFNOHISdDgEXpgIFfsAOLO8gn9kZ1HqCG2+o8Th4OpOeZxdVs54TxkD6wlp2+wcngvUvVZkJLk4Z0j9M2oSP1r43hpUlsALk2DjQvzABynJPJ+WytKk/WHGtG2O81ZyhsfmP8W/5Bu7FwC9jR28776NxVZ/XgyeyMfWIHbToe45DB/O1LUYzjLAxg6mECjrC1Z408cuh8HEY3rwhzEDcGjbBxGR8PgqYOVs+GIa7FzB3dlZPJvR+Jqshhy65VQGV1qk4sVLAhvsTiywjsCq5ybVL35yKL88rW+0o9f1O0wKWS3NXwlPjoVtn7Eiwc0teTkUNLLfCUBXX5CKbZezuepwxpqLWG334Du7S711Dfce3Fmf4spYhuGorHXMtlz4PUfgLxyBVVV/+4acNqAjj00apqAlIhKpwg3s3LeW8Z//hUJ/adP1DxCo6IF3888IZdXPWYfn89CEI2OyH6Ku3+FRyGpp7/wGFj/CZ4kJ3NAxl8p69kSpT0rQpmDzL7CqGp7+dWYsJbHTSxhG42uqbNugquBM/IWN70j8Y1eM6MGfztXTKiIi0Vi5ZyU/e/9nlPpCC1rBynwqNl8DVtN7G048pjt/PndQzH4h1vU7PFr43pKqyuDLmWx3Org5L/SABVDuMEjq9gSYFfUed2YsJanz7CYDFoBh2CR2fBt3h/khnx/gqcUb2VLoCauNiIjUdnju4Tw9+mkGdBjQaD3bNvB7BlOx+Vo6p2Vz6dHdGT0oH7ez9rUjLcHJFSN68P4tJ/HX8w/XHYcWpIXvLemrF6DKwzPZmSEvfDyQ6fLgyvwCf+FJtcoN9x4SO70Udn/u3HcJVvQk6O0ZUn3bNrli1n9596c34XboPYgiIpE6JPMQXhjzAl/t+YoX1r7A/K3zKfWVYhomucm5nNH9LI7PH0OaI4+MJBedM5NqwlNxhY8thRVU+IKkJjjpnZtCsluX99ZAfwst6euXqTQMXk2NfJd1d9YS/IUnAsYBZZ+GNIP1Y4Zh48r+hOD2niG32bStC3/4ZCr3nnRP2OcTEZHaBucOZnDuYAAs28I0mv4FPDPZTWayftFtjXS7sIWU+krZs+db5iYn4XE4Iu7HdO/Dkfzd/gLDhytjWcT9udJW4UjaAGZl05UBO5DOm+s/YEPJhojPKSIidYUSsKR1099gC9hetp2Jb02kuKKKra7wd2b/MdO9r+bPztS1dZ4iDIthk9zjP6QeeheJnZ/HTNrcZBPbcjNr7azIzykiItIOKWQ1s33efVz17lVsLNnIXiubSiMGCxIN//4/OsN/t9WPdQkEMMwArozlpPR8lKTu//1+gX39D6IaZiWvrX8NXyPv6BIRETnYKGQ1s78s/gvby7YD8LZ1FGlWw6/LCZVtxXYH9v4+f63PzpQNJPd8DEwvjrQVGM79TxSa7gIMh49Sfyl7vJG9d1FERKQ90sL3ZrSrfBfzt86v+fwSR/Ef71shtbVtg2DZYfiKj8aqygXbjWFWYiZuI1Dee3+9YNP7pjTFCyRbFhUHbCnhSCggqetMLF8Ork6v4N16OUFvL1xZi2vqVPjr305CRETkYKSQ1YxmrZ1F0N7/1J/X5eXbihEc6vuKb90NPxniLz6Sqr2nYfuzapXbpGP58mqVBcr64rbAF8Uc5ScpySTVM8PmTNlAwHJiOLwkdZ9OxZYrcGUsrTme5o7s1RAiIiLtkW4XNqMPt3xY67Mr8wvuCUzg+JKGb/dV7RlF5c5L6gSsBllJdNgxCr/ncGw78r9ebwMbo5pJ1bc6DdNPcvcnMRxVAOQl55GblBvx+URERNobzWQ1o6KqolqfnalfMcTZm+4lnRmcupGvkmo/aegrPBbf3lFhn2dd6SgoBcPhwZX5Oa7sRZjO8qjG/gPDsb8fw9y/duuivhfhMCPfikJERKS9UchqRjV7ntg240rLmOgppZfzfgBOLzC5Nj+XVQkJ1VUsN1V7zozqfHYwHd++n+AvGUZStxk4EndH1R9AfQ9DOk0nFx16UdR9i4iItCe6XdiMcpJycNg29+zZx+/3FdHLH6g5lmFZTN9ZwGUlHtKCFv6SoRCjpwbtQCbeLT/F8mXHpL8fG99vPLnJulUoIiJyIIWsZnRmzzP5w95Czi6v/ym8JNvm14XFfLB1O9l7hsf03HYwFe/28THt8wdn9TorLv2KiIi0Zbpd2IwucXUkrazptVGG5WRnsGvMz29Vdifo7YLj+8XrsWI3sEmpiIjIwUwzWc0obflzIdUrIyluY/AVjYh5n1mJIT75KCIichBRyGounh2w9u2QqiZTFbdhBEoHxrS//tn96ZbWLaZ9ioiItAcKWc1l40dwwEakjUk2quhCnF5RYyVFtX/Wj43rNy5mfYmIiLQnClnNpbI4rOoTnB82XSkiFg296DlcmQmZnNVbi95FRETqo5DVXBwNvzanPuMc83Djb7pimAxHBYYRfchymS7+efI/SXLGb/2YiIhIW6aQ1Vwyw1u3lGt4uMLxbsyH4UxfEVI9y59OwNu53mPJzmT+feq/ObrT0bEcmoiISLuikNVcep8C6V3CavIb53OcZS6J6TBCCVlBbxcqNl2P5e1Zq9wOJjGu70TmjJ3DCV1OiOm4RERE2hvtk9VcTAcMmwzz/hp6E8PmIde/+at/ItOs2Kx98m6+Bmf6StxZi3Ekb64pt20HgdIB+IuOJVhxCAB+z2DsYCLYTix/FjcMP49fjhgUk3GIiIi0d4Zt282+k6TH4yEjI4OSkhLS09Ob+/Qtp6wAHj4GvIVhNz208kn8uJquGA6zAsNRAZjYgVSwG143Nn54N+65cHBszy8iIm3KQXv9jpBuFzan1DyY8ByEsVg8ALyXnMRgY0Psx2MlY/tzsP3ZDQasJJeDX47qq4AlIiISJt0ubG7dj4UrXofnL4XygkarlhkGv87L4aPkJLqZn8Hefs00SHA5DO4Y3Z+LhnUlIynGM2giIiIHAc1ktYRuw+Hmr/Cc+TfWJSbXObzB5eTu7CxO696Fj5KrZ722dFiD4ShrtiH+7qz+XHVCLwUsERGRCGkmq6W4kkg/9gY8A8/lmjcn4y3ZgolNkelgo7tusDHMIH0OWcm368J996ANGGG16JOXysVH6VU5IiIi0dBMVgvrmtaV+y98jbNO/hMlHQfWG7AGdRjEX47/C29eMZXbzzws5L4dBtx2WCGdHcUht+mSmcQTVw4nJUH5W0REJBp6urCVWbp7Kd8Vf0eFv4JkVzIDOwxkYE7tlzq/unw7/3hvLVsLvQ320ycvld+edRinHtaR3SVebn1qIR9vDzR67pP65vKPiweTl5YYk+8iIiLti67f4VHIaqMsy2bBuj08s2Qza3eXUlEVJDXRyaDOGUw8tjvHHZJTp836glJmLt7Cmyt3UlTuAyA7xc1Zh3fishE9OCQ3tbm/hoiItCG6fodHIUtERERCout3eLQmS0RERCQOFLJERERE4kAhS0RERCQOFLJERERE4kAhS0RERCQOFLJERERE4kAhS0RERCQOFLJERERE4kAhS0RERCQOFLJERERE4kAhS0RERCQOFLJERERE4kAhS0RERCQOFLJERERE4kAhS0RERCQOFLJERERE4sDZ0gMQEZHWbVvpNjZ5NlEZqCTFlUL/7P5kJma29LBEWj2FLBERqSNoBZm/dT7Pr32eJTuXYGPXHHObbs7oeQbjDhvHEblHtNwgRVo5hSwRkXaksNzHi0u38W1BKRW+IGmJTo7omsm5Q7qQ5HaE1Mde715+/sHPWbVvVZ1jbsvm1LIiOi6byadLn2Z77uGcdspduDofGeuvItLmGbZt201Xiy2Px0NGRgYlJSWkp6c39+lFRNqdb3eX8uj873hj5U58AavO8bREJxce2ZXrRx5CXnpig/0UVRZx2duXsdmzGQDbNjAMm87+ABM8pZxXVk6mVbd/u+twjOFXw+EXgxlamJO2R9fv8ChkiYi0cQvW7eH6mUsp9wWbrNsxPYF7LxxM/07pZCW7cTtrP//00/d+yiff7cBXdCyBssPASuR4cyWPuh4g3ahsejB9ToOLn4CE1Ai/jbRmun6HRyFLRKQNW7q5kEsfX0JVPbNXTUlxOzh3aBcuH9GDw3KTeWfeDK7/yMaq6lJT5yhjDc+47ybB8Ifeca+TYdKL4HCFPSZp3XT9Do9ClohIG2VZNiP/MZ8thRVR9zXMuYEVgXwCJNeUJeDj44SbyDU84Xc48jcw8o6oxyWti67f4dHCdxGRNmrBuj1hBiwbR8q3uLIW40zeCGYl2E4sXzZfFQ8nUNIJDpgQG2MujixgAdYX0zFPvFWzWXJQU8gSEWmjnl68OeS6juTvSMx/GTNhb+0Dhh9H4m4c+W+QkPcuvsLj8O05AzC5zDk34rGZZbvhm9dh0AUR9yHS1mnHdxGRNsi2bRas2xNSXWfaSpK6Ta8bsH7EMP0k5CwgsetM8tnDEPO76Aa5+pXo2ou0cZrJEhFpgzyVAYJW00tqzaTNJHZ+AcNs+snDH7jSVmPkA8WRjw+grGg3esZQDmaayRIRaYNcS6eFVC8hdy6GGQi7/7LM1Wx1Rvd7eKU//CceRdoThSwRkbZk6+fwzMUkv387KXgbrWq6C3CmrI/sPAa8kB7dPFSZMy2q9iJtnUKWiEhbUFUKz1wM00bBt+/xabA/XtyNNnFlLonqlHNS0/FF0f61ZM1kycFNIUtEpLWrKoMnx8K371V/tJ383H8TFo2/vsZM3BnVacsdsDvCW4a7HQ6eYjP7vPuiGoNIW6aQJSLS2r1yLez4subjm9ax7CWjyWaGGc08VLUtRtPnqc9z6alU2QFe+valqMcg0lYpZImItGZ71lXvN3WApwOnhdTUthKiPv1dVVOosMPrZ0FSIjMyqncDn7s58r22RNo6hSwRkdbs8//V+lhgZ/ClfWhITa3KTlGd2gok843/cC7z3UGhHdoi+PeSk7glLxfLMADYWrolqjGItGUKWSIirZUVhBXP1yoqskN/Ys9XfAy2bUR8+kDJUWA7WWr344yqe7k/cAG77Ky6wwQ+TUzkF3k5/CovB5+5/5yp3hI2L7wn4jGItGXajFREpLXyFkNVSa0iB008sWd6caauwkwowKrKI1h+CM7U8LdxsG0DX9ExNZ/3kMX9gYt4MHA+I83ldDP2kICfqvz3+CLJzWZX/e8ozA0E6fHh3eBIhuNvCnscIm2ZQpaISGsVqKxTtNPOAmygnhkq04srazHurE8xXdUvdvbuHIMjeVPYG5L6i47B9neoUx7EwQfWsO/PV0la+ieN9nNmefULrO25UzEyu8HA88Mah0hbptuFIiKtVWLtJ/vu81/CZf7fUW/AArCS8O87hfINtxAo64ttGwRLj6By+wRsq/HtHg7k9wyiavc5TdZzpqxt9HiSZXFeWRkABjZ8eBfYTb8KSKS9UMgSEWmtElKh85EA/M1/KQ8HzwutnZWId+tkytb9BjuYRqBsIN6tVxGsymu0mR10U7X3FCq3X0oolwdX1uJGj48pKyf9wPcr7lsPG+aH8AVE2gfdLhQRac2GX827L03nv8ExYTY0wUqv+RSs6E3FhltwJK/HlbUER/IGDEclWE4sfzb+4uH4S44EKzG03hN24UzZWKvMDrqxfHnYViJpfhfnF64Eo6h2w6Uz4JBTwvwuIm2TQpaISGs26EKmvbgnZt0FK/oQrOgTZS9VJHaavb/Pyo74i0bg9wyF7/fm8gJjsRlpruBnjtcZ4fimunLBmijPLdJ2KGSJiLRi6wr9fBYIbV+s+gWI9Y96V9bnOJK2Y9sGVbvOxV98bAM1DeZbQ5hvDeGE4Eoecd1Puq88pmMRac20JktEpBV7bfmOKHuI/e/SwYo+2LZB5fYJjQSs2j62DmeC7/eUueo+sSjSXilkiYi0YntKq6LvxKy7FUQ0rKp83DvPJFA6OKx2X9u9uK3i8piORaQ1U8gSEWnFrBhseeBK/7LpSmH6U/kKbnS8HHa7d4o6sWmvbhnKwUEhS0SkFctOcUfdhzN9JQl5b8VgNPtZhsGvXLP5k3NGWO1sYObizTEdi0hrpZAlItKKnT6wY1TtDUcpjuTNuDssJLHLMxju2DypmE71Tu5XOOcyxfF2WG3nLNsWkzGItHYKWSIirdiwHtkM6JTedMUGuDK/wDCC1X9OX0lK73/izn0nqjElU8lR5rqaz9c5X8VF6K/tKa7wU14V3mt+RNoihSwRkVbu8hE9ImwZxJW1pFaJYYC7w3wM196Ix3Ou4xPSDG/N51zDw2hzSSMt6vIFmnjRtUg7oJAlItLKjRvejbMP7xR2u8ROL2O6iuuUGwa4m3glTmMuc8ytUzbBMS/k9oYB6UmuiM8v0lYoZImItHKGYfB/445gzOBQg1aQhPyXcWV+0WANV9anOJI2Nni8Idc7XmWAuaVOeU9zV8h9HHdIBxxmAy+5FmlHFLJERNqABKeDBycM5YHxQxjWI6v+SkYAZ/pykns+ijur8dt3hhkkqdtTOJI2hTyGKx1vc5vzhXqPJRH6fl6XHRvp7U+RtkWv1RERaSMMw+DcIV04d0gXVu/w8OGa3ezweHjx21ngLMaZvgLTGfoeVIbDS1L3/+HbdzL+4qOxAxn11uttbuIXjjc417Gowb5K7eSQzpmfnshpA/JDHqNIW6aQJSLSBg3onM6AztVPHeZ8sYAZX0e2D5ZhBkjI/QB3zjwCpQMIlPXHDiZjGEEcDg+3Wh9yjW91k/0stZt+v6LLYfCPi4/QrUI5aChkiYi0cZMGTOLV716lsLIw4j4Mw8KVvgpX+ioABlRV8et9xQzzhXYb8OnAaY0eT3Ca3D9uCCccmhPxGEXaGoUsEZE2Li85jwdPfZCfzf0ZZf6yyDqxbboFAgytrGK8p4zDfb6Qm662erDU7lvvMYdpcOphedx4Sh+O6JYZ2dhE2iiFLBGRdmBw7mCeOPMJfrXgV2zybAq/A8Ogv8/PX/YWhvVEVJXt5L+Bs3jWdRff2D1YYR1CKckkp2ZwyLBRjD+mJ50zk8Ifj0g7YNh2DN4+GiaPx0NGRgYlJSWkp0e+k7GIiNRm2zaf7PiEF9a8wIJtC7AJ70f8JZ5SfruvGEcI7fy2g/VWZw4zt2IcuMzqiEth7APgjP69i9K66PodHoUsEZF2yhf0MWfdHP7+2d8JEgytTdExHFfQk185ZzHQbORFzqYLLP/+zwkZMORSGH4V5DS9CF7aJl2/w6PbhSIi7ZTb4ebS/pfSPb0793x2D5s9DYcmO5BM1b6R+AtPYh4wzzeUI411XOr8kMHunfTOAGdiGnQeAkddBbn9wLMdqsrAnQJpncCV2GzfTaQt0EyWiMhBwLZtPt35KS+seYEVe1ZQ5i8jyZlEXlJXkiuPZ9nqHpRX7V+NZRjVO7NfdmwPRvXviNOhvatF1+9waSZLROQgYBgGx3U+juM6H1fv8dJKP6t3eCitDJDoctCjQzLdskPbYFRE6qeQJSIipCW6OKZ3h5Yehki7ovlfERERkThQyBIRERGJA4UsERERkThQyBIRERGJA4UsERERkThQyBIRERGJA23hIK2fFYQdy8FbCIZZvbN0xwEtPSoREZFGKWRJs/iu+Dv2ePdgWRYZiRkclnUYDtPReKPSXbD0SVj6BJTuqH2s4+EwfAoMHlf9Sg8REZFWRq/VkbjxBry8ueFNZq2dxTeF39Q61imlExf3vZgLDr2ADkn1bIC4/Dl4/SYI+ho/SUouTHgBug6L4chFRKQ+un6HRyFLYq6kqoRHlj/CC2tfIGgHG62bhpM7h/+a0/teCE53deEXM+CNm2vVq7RdmFi4jXr6cyXD5a9R2elwVu9bTZm/DKfppHNKZ3pm9IzNlxIREV2/w6SQJTFTFazivs/v46VvX8Jv+Ruslx0MckFpGReVltElcEBo6ng49PkJfPJvwGKpdSgzA6N41xpOBYkAZFDGWMenXO54j77m9pqmFe4kLujeg+3BilrnGpI7hHGHjeOMHmfgcrhi+n1FRA42un6HRyFLYqLCX8G171/LlwVfNljHsG1+WVTMxJJS3I30ZdkGjwXG8PfghEbPeby5kvtdD5NreABY6XbzYFYGnyYlgmHUqts9rTuPjnqU7undQ/5OIiJSm67f4dEWDhI127a5Zf4tTQasv+/Zx5VNBCwA07C53vU6v3C82Gi9T6zDGRW4i1VGDgCH+3z8d/ceXt+2k/GeUjjg94ctpVs456XxTF/yOb6AFfJ3ExERiZRClkTtj4v+yCc7Pmm0zi+LijmzvKLROnXauF7kAnNho3VKgjmcE/wtz6Zm1ZT1DAT43b4i7t2zD8cBQStolPKPFbcz4p65PPXpprDGIiIiEi6FLImYbdvcuehOXl7/cqP1soNBJpaURnSOXzpfxKDxmSfLl8+fnKfyVHparfKzyiv4497CWmWOhD2UGMuZ+urX/PXN1RGNSUREJBQKWRKxh5Y/xIvfNn5LD+D80rImbxE2pJu5h5Hmiibr+YuO5R/ZmSxOTKh97rJyRni9tcpcmYsBePyjjfzvow0RjkxERKRxClkSkb3evUxfNT2kuheVlkV1rksdHzRZx6rqQqCyGzMy6i7EvLrIQ4K1fzbMkfIdhrMEgH/NXUdZVSCq8YmIiNRHO75LRF5c9yIBq3Y4CVZ1wJGwr1aZ27LpGmh8r6ymHGLsaLoS4N0+gcVGGZcHChljfsY5jkVU4WJT+XDGb8jia3ci3yQFKM1ch+H0YAcyKPcFeWnZNi4f0TOqMYqIiPyYQpaEzbIt5nw7p065v/gozNwPMMz94SvJjv5JvhSjMqR6tr8DJXRgIT1YaA3ld4Ep2BgEfvi/uR8oB2OvhW3u7/OB979l/PDuuJ2a2BURkdjRVUXCVlhZyK7yXXXKbX8HAp7BtcrKzej/L1ZqJ0fUzo9rf8A6gI0J1v4+95X7mDRtCZ7KhjdQFRERCZdCloStwt/QVgwmvqIRtUoChsE37tB2Wq+wE3g+MJI/+Cdzi+86fu+/kqcDo1hiHRbliJv22cZCfvbUUu2hJSJyEFq4cCFjx46lc+fOGIbBK6+8EpN+dbtQwpbkTKq33HCUY5UOwrfvJNwd9u9vNSstjT/uK6y3DcBOO5v/BMbwYvBESkmpc9xsYguHWPl0wz6eXbKZycf3apbziYhIXUHL5rONhRSUVpKXlsjRvbJxmEbTDaNQXl7OEUccwZQpU7jgggti1q9CloQtOzGbDokd2FdZe5G7M3U1/uJjqCoYDYYfd/anALyZmswvi4pIt+q+wWmV1YMrfb9mD1l1jv3AasYJ15lLtihkiYi0kHdW7eRPr69mZ8n+dbOdMhL549gBnDmoU9zOO3r0aEaPHh3zfnW7UMLmMB1ccGjdpO9IXYfh2gcYVO0+F+/2cQS9XfCaJvdnZdapv9HK53LfbxoNWM1tfUEZi77b29LDEBE56LyzaifXzVxWK2AB7Cqp5LqZy3hn1c4WGlnkFLIkIhf3vRiH4ahVZhg27qzFNZ8DnqFUbPo55Ruv55nAGTyQ1LdW/dv9P6WQ1veC0YXrFLJERJpT0LL50+urqXu/g5qyP72+mmA9d0RaM4UsiUin1E6M6zeuTrkr+xMcKetqlVmV3anafQ7/KrqTX/quY4uVyxqrG5/Z/ZtruGEp8eopQxGR5vTZxsI6M1gHsoGdJZV8trHh9b2tkdZkScR+PfzX7Czbybxt82rKDMMiqevTeLdNJFhe96nAl60TecV3PJ3YV+dYa5Gg/bJERJpVQWlo+yGGWq+1CPtq4vV6+fjjj1m9uu7LdSsrK3nqqadiMjBp/RymgxuPvLFOuWH6Ser2JAn5L2Im1N2t3cZkBznNMcSIdM5MbOkhiIgcVPLSQvu5G2q91iKsmax169Zx+umns2XLFgzD4IQTTuD555+nU6fqFf8lJSVceeWVXH755XEZrLQ+lYH6f6uoXp/1Oe6szwlWdCdQ3hc7mARGANNVQlXBGWAn1Nu2JbkcBucN6dLSwxAROagc3SubThmJ7CqprHddlgHkZ1Rv5xAPZWVlrF+/vubzxo0bWb58OdnZ2XTv3j3ifsOaybr99tsZNGgQBQUFrF27lrS0NI4//ni2bNkS8QCkbUtx1d3X6sccyVtIyH2f4Zlz+K3rWf4efJ0kWudLmU8fkE9eetv6TUlEpK1zmAZ/HDsAqA5UB/rh8x/HDojbfllffPEFQ4cOZejQoQDccsstDB06lKlTp0bVb1gzWYsWLeL9998nJyeHnJwcXn/9da6//npOPPFE5s2bR0pK0xdcaV+6pHYhzZ1Gqa+0wTpjS8uY6CljoM9XU/YQRXxbz8ajLclpGlx9ovbIEhFpCWcO6sSjk46ss09WfjPskzVy5EhsO/ZPLoYVsrxeL07n/iaGYfDoo49y4403cvLJJ/Pss8/GfIDSuiU6EznnkHN45ptn6hwzbZs79xZyfll5nWPnOz7i74EJzTHEkP31/EEM7d569uwSETnYnDmoE6cNyG/2Hd/jJazbhYcddhhffPFFnfKHHnqIc889l3POOSdmA5O2o76tHAD+sK/+gAUw3jEfN756j7WEQ3JTGDc88vvuIiISGw7TYMQhHTh3SBdGHNKhzQYsCDNknX/++Tz33HP1HnvooYeYMGFCXKbbpHXrldGLizMH1So7ocLLRaX1ByyAbKOUCx0fxXAUtf9/14m99Dc2kUhVSK3P1WJ3ERGJMcNugVTk8XjIyMigpKSE9PTWt+O3hC/wyvXctuNd3k9JBuChXQWc7G18P5NK28VE329ZaveL+vw92cEmOkfU1uUw+OSOU9vco8EiIs1N1+/whL1P1qZNm3j88cd5+OGHWbVqVTzGJG2Q01fOPwv28tPiEvr4fJzQRMACSDT8POW+h5Hm8qjOnWKU8aDrIdJoeOasMacPzFfAEhGRmAtr4fu8efMYM2YMXq+3urHTyfTp05k0aVJcBidtiDsFE7ipqIRri0pwNNmgWopRxQzX3/nIOpyng6fxoTWUYK3WNnUf6N3PcJTzd/f/cbi9ib8xjZv8N2KH8btDfnoivz+7db7eR0RE2rawbheecMIJ5OTk8Oijj5KYmMjvf/97Xn75ZXbsqLurd2M03dgOLXoQ3vt91N3stLNZY3WjjCRSqSTDKOV6383sokPtikYAZ9oqjkh/kxf3fl1T/FzgFH4fmPKjoFa/ThmJPDnlaPp2TIt63CIiBwNdv8MTVsjKzMxk0aJFDBhQvWFYRUUF6enp7N69mw4dOjTRej/9JbVDFYXwf/2hgR3go7HE7MSl5lVgJWCYlZhJW3Flfo7pLOdvBXsZW15Rq/6i4AAeDp7HImtAvbNayW4H5w7pws2jDqWjNh4VEQmZrt/hCet2ocfjISdn/zvnkpOTSUpKoqSkJKyQJe1QcjYMOA++ej7mXR9j7WRI/v9Yl+CuVd7FH+CMHwUsgOMcqznOsZrvrE48wqm86u6JbbtIMCr46bATufqUEaQlumI+ThERkQOFFbIA3n33XTIyMmo+W5bFBx98UGsRvPbLOkideCusfQuqPDHvenxpGX9O2P/OqoxgkEd2F+BupM0h5k5udL3Ae107kxcI8EjKIPqN/l3MxyYiIlKfsG4XmmbTC4oNwyAYDDZaR9ON7diG+fDcBPDXnWGKhsc0OL5HN6B6BuuR3QX09jf9/sM1LhfvpyQz3hsgZ/I70GlwTMclInIwaY/X77vvvpuXXnqJNWvWkJSUxHHHHce9995Lv37Rby8U1hYOlmU1+U9TAUvaud4jYfIbkNUzpt2mWzbDvJXcXbCX17ftCClgARzq93NjqZecC6YpYImItAVWEDZ+BCvnVP+vFd9csWDBAm644QYWL17M3Llz8fv9nH766ZSXR7Yt0IFiuhmpZVm89dZbjBkzptF67TEJSz0+/Css/HvLjiE1Hy6aBj1PaNlxiIi0A3G/fq9+Dd65HTwH7FqQ3hnOvBcGNM9SpD179pCXl8eCBQs46aSTouor7M1I67N+/Xp++9vf0rVrV84///xYdCntwam/g3HPgDOpZc6fcyjc8o0ClohIW7D6NZh1ee2ABeDZWV2++rVmGUZJSQkA2dnZTdRsWsQhy+v18tRTT3HSSSfRr18/Fi1axNSpU9m2bVvUg5J2pP8YuPmr5g86DjeMfRBCWEcoIiItzApWz2BR382178veuSPutw4ty+Lmm2/m+OOPZ9CgQU03aELYV6DPP/+cn/3sZ+Tn53P//fdz7rnnYhgGjzzyCNdeey0dO3aMelDSzqTmwfn/BSPUfeDr2tVzROiVHW644HHoEUYbERFpOZsX1Z3BqsUGz/bqenF0ww03sGrVKp5/PjbbEYUVsgYPHszFF19Mhw4dWLRoEcuWLePWW2/FMBp+7YkIABldqme1ItHtGPInvwNn3E0wKavxunkDYdJLMPC8yM4lIiLNr2x3bOtF4MYbb+SNN95g3rx5dO3aNSZ9hrVP1tq1axk3bhynnHJKza7vIiEbcz/sWgWF34XeJrVj9awUwIjrcQy/Gla/AkufrO7HXwEJGdDtaBh+tWavRETaotQQ74KFWi8Mtm3z85//nJdffpn58+fTq1evmPUdVsjasGEDTzzxBNdddx1er5cJEyYwceJEzWRJaJKzq7d3eOZi2L2q6fqZ3WHii5DVY3+Z0w2DL6n+R0RE2ocex1U/RejZSf3rsozq4z2Oi/mpb7jhBp599lleffVV0tLS2LVrFwAZGRkkJUX34FbEWzh8+OGHTJ8+nZdeeonKykp+9atfcfXVV9O3b98m22oLh4Ocrxy+nAmfT4O9a+sez+wOw66EYZOrg5mIiLQKcb1+//B0IVA7aH0/kXPJU3HZxqGhiaIZM2YwefLk6PqOdp+skpISnnnmGaZPn86yZcsYNGgQX331VaNtFLKkxsaPYOdyqCoFdwrk9oc+o/RUoIhIK9Qy+2R1gTPvabZ9smIpppuRLl++nOnTp/Pvf/+70XoKWSIiIm1Ps1y/rWD1U4Rlu6vXYPU4DszIn05vSTENWaFSyBIREWl7dP0OT1gL30899dQm6xiGwQcffBDxgERERETag7BC1vz58+nRowdnn302LpcrXmMSERERafPCCln33nsvM2bMYPbs2UycOJEpU6bEZNt5ERERkfYmrEe4brvtNlavXs0rr7xCaWkpxx9/PEcffTSPPfYYHo8nXmMUERERaXOiWvheUVHB7Nmzefjhh1m9ejU7duwIaSGcFs6JiIi0Pbp+hyeqzYiWLVvGggUL+Oabbxg0aJDWaYmIiIh8L+yQtWPHDv72t7/Rt29fLrroIrKzs1myZAmLFy+Oevt5ERERkfYirIXvZ511FvPmzeP000/nvvvu4+yzz8bpDKsLERERkYNCWGuyTNOkU6dO5OXlNfpS6GXLljXaj+7pioiItD3t8fr96KOP8uijj7Jp0yYABg4cyNSpUxk9enTUfYc1DTV16tRGw5WIiIhINIJWkGUFy9hTsYfc5FyOzDsSRxxfq9O1a1fuueceDj30UGzb5sknn+Tcc8/lyy+/ZODAgVH1rdfqiIiISEjiff1+f/P73PPZPeyu2F1T1jG5I3ccfQejeoyK+fkakp2dzX333cdVV10VVT9hLXzPysoiOzu7zj+9evXijDPOYO7cuVENRkRERA5O729+n1vm31IrYAEUVBRwy/xbeH/z+3EfQzAY5Pnnn6e8vJwRI0ZE3V9Ytwvvv//+esuLi4tZunQpY8aMYc6cOYwdOzbqgYmIiMjBIWgFueeze7Cpe3PNxsbA4N7P7uWUbqfE5dbhypUrGTFiBJWVlaSmpvLyyy8zYMCAqPsNK2RdccUVjR4fMmQId999t0KWiIiIhGxZwbI6M1gHsrHZVbGLZQXLGJ4/PObn79evH8uXL6ekpIQ5c+ZwxRVXsGDBgqiDVlSbkf7YmDFjWLNmTSy7FBERkXZuT8WemNYLl9vtpk+fPgwbNoy7776bI444ggceeCDqfmMasqqqqnC73bHsUkRERNq53OTcmNaLlmVZVFVVRd1PTHcSnTZtGkOGDIlllyIiItLOHZl3JB2TO1JQUVDvuiwDg47JHTky78iYn/s3v/kNo0ePpnv37pSWlvLss88yf/583n333aj7Ditk3XLLLfWWl5SUsGzZMtatW8fChQujHpSIiIgcPBymgzuOvoNb5t+CgVEraBlU7895+9G3x2XRe0FBAZdffjk7d+4kIyODwYMH8+6773LaaadF3XdY+2Sdcsop9Zanp6fTr18/rrvuOnr16tVkP9onS0REpO1piX2y8pPzuf3o25t1n6xY0WakIiIiEpLmuH43947v8aS3O4uIiEir4TAdcdmmoSXE9OlCEREREammkCUiIiISBwpZIiIiInGgkCUiIiISBwpZIiIiInGgkCUiIiISBwpZIiIiInGgkCUiIiISBwpZIiIiInGgkCUiIiISBwpZIiIiInGgkCUiIiISBwpZIiIiInGgkCUiIiISBwpZIiIiInGgkCUiIiISBwpZIiIiInGgkCUiIiISBwpZIiIiInGgkCUiIiISBwpZIiIiInGgkCUiIiISBwpZIiIiInGgkCUiIiISBwpZIiIiInGgkCUiIiISBwpZIiIiInGgkCUiIiISBwpZIiIiInGgkCUiIiISBwpZIiIiInGgkCUiIiISBwpZIiIiInGgkCUiIiISBwpZIiIiInGgkCUiIiISBwpZIiIiInGgkCUiIiISBwpZIiIiInGgkCUiIiISBwpZIiIiInGgkCUiIiISBwpZIiIiInGgkCUiIiISBwpZIiIiInGgkCUiIiISBwpZIiIiInGgkCUiIiISBwpZIiIiInGgkCUiIiISBwpZIiIiInGgkCUiIiISBwpZIiIiInGgkCUiIiISBwpZIiIiInGgkCUiIiISBwpZIiIiInGgkCUiIiISBwpZIiIiInGgkCUiIiISBwpZIiIiInGgkCUiIiISBwpZIiIiInGgkCUiIiISBwpZIiIiInGgkCUiIiISBwpZIiIiInGgkCUiIiISBwpZIiIiInGgkCUiIiISBwpZIiIiInGgkCUiIiISBwpZIiIiInGgkCUiIiISBwpZIiIiInGgkCUiIiISBwpZIiIiInGgkCUiIiISBwpZIiIiInGgkCUiIiISBwpZIiIiInGgkCUiIiISBwpZIiIiInGgkCUiIiISBwpZIiIiInGgkCUiIiISBwpZIiIiInGgkCUiIiISBwpZIiIiInGgkCUiIiISBwpZIiIiInGgkCUiIiISBwpZIiIiInGgkCUiIiISB86WHkBr9e3uUpZvLaasKkCy20GfvFSG9chu6WGJiIhIG6GQdYBA0OLtVbt4evFmPttYWOd4v45pTDq2Oxcc2ZWUhPr/1fmDFi6HJghFREQOdoZt23Zzn9Tj8ZCRkUFJSQnp6enNffp6eSr9XPv0UhZ9t6/Jur1zUnjiyqPp3iGZoGUzd/UuZi7ewhebC6n0WzhNg545KYwf3o2Lh3UjI9nVDN9AREQkvlrj9bs1U8gCvL4gEx5fzPKtxSG3yU9PZNLxWTw6fwvl3oZDVKLLZPJxvfj1Gf0wTSMGoxUREWkZre363drpvhbw17dWhxWwAHZ5KvnH2zsaDVgAlX6LxxZ8x43PLSNoNXueFRERkRZy8K3JKtwIX86EfeshUEmJowNzvjoLiGSWqW6bJCrJNUpIwI/HTmY3WYDBWyt30TF9NX8cOzDabyAiIiJtwMETsrYshoX/gO8+ANuqKZ4dGE1lMPrbeEOM9VzmnMvZ5mISDX9N+XdWJ54JjmJO8CSe+nQz15zUm04ZSVGfT0RERFq3gyNkLX8OXrsRrECdQ68HR0TVdTplPOJ6gBMcX9d7/BBzJ1PNp/mVcxZ3Bi7n2SWHcOvp/aI6p4iIiLR+7X9N1jevw6vX1xuwAPbaGRF3nUEZc9x/ajBgHSjZqOLvrscxFz+stVkiIiIHgfY9k+X3wqs3YtsWBQ4HZaaB27bJCVokff9QpR3RWqxqj7nup6+5Paw2vwg+xaxnD2dv55F0ykjijEH5pDaw55aIiIi0Xe366u5ZPpNX3EFmd+jEJvf+pwATLYszyysY7ykjq6qUHXZO2H0PN9YwwrE67HamYdN33WPcsaozAH987WvOG9qZycf1pE9eWtj9iYiISOvUbm8XvrjuRUZ9/W/u65BVK2ABVJomr6SlMr5LPlXp4QclgMuccyMe25HmegYamwAoqwowc/EWzvr3x7zx1Y6I+xQREZHWpc3PZG0v9vLsks28unwHBaVVWJZNasePCWa+HtKuDDtyv4KS0YSTN9Mp4wzz88gHDYxzzGNq4Mqaz76AxU3PfYnTNDlzUH5UfYuIiEjLa7Mhy1Pp5zcvreSdVbtqLSR3pK4mkPFGSCutbBt8BeEFLIDORiEJRvVC+krbxUfW4eyzq3e+7WB4OMFcRZLha7SPHsbuOmWWDTe/8CUf9zyVnNSEsMYkIiIirUubDFmF5T4ufXwxa3aV1jmWkPMhhtH003tO2+awok4kltkEzRXssrNZZ3cL6fyJ+Nho5fN0cBRzgifjIaXW8XTKudCxkMscc+lt7qq3j6PMtUx1PsXTwdPYaHeqKa/0W7zw+VZuOKVPSGMRERGR1qnNvbvQH7QY959PWbaluM4xM3ErKb0ebrR9J3+AS0rLOL+0jA6WVevYKqsnM4OjeCV4PJU0PJOUQxHFpBFoIqM6CfA35zQucS5osI5lGyyyBvDHwGS+s7sA0CUziY9+fYredSjSVlWWwPJnYcXz4NlevYVMYiYcejoMvxpy+7b0CEUioncXhqfNhaw3vtrBjc9+We+xhPyXcWctabDteE8pt+8ranL6bpedxVW+2/ja7hnW2Bpyt/NxJjjnNVqn2E5hsu92ltvVM1gvXX8cR3bPisn5RaSZBHwwdyosexL8FQ3X63USjH0Asns339hEYkAhKzxt7unCpz/d3OAx072vwWOTiz38LoSABZBvFPG8+y81TwBG6w+BK1ltdW+0TqZRznT33+lhVN9eLCxrfE2XiLQyfi/MvACWPNp4wALYuBD+dxrs/Kp5xiYiLaJNhaz1BaUs2VjY4HHjgHcGHmiE18svi4rDOlea4eVZ912cb37EIGNDWG1/LICTJ4NnNFkv2yjjl845ABi6UyjSKvmDFm98tYNJ/1vCcXd/wNA/v8eJf/+Qrx64CDZ9FHpHFXvhmYuhJLwNjUWk7WhTC9+/2Vl3ofuBbCux3vIrSkojSpMZRgX/cj8KwGqrBzODo3g5eDxe6j9PY14LjuC3zmfIMBr/DXe0+Rl/xkMHPV0o0uo8vXgzD37wLQWlVbXK+1WuYLD74/A7LNsFnzwAZ/09RiMUkdakTc1kVfjqf//gD4IVveqUdfP7GeGtjPrcA8zN/M01jbkJv+YQI/zfPL0k8l7wqCbrJRgBxjs+5M2vthMIWk3WF5Hm8dc3V/OHV1bVCVgAkxyRb07MiufAVx7FyESktWpTISuliXf8+YuPwrYctcrGlpU3+CUrbRfb7By+szqx1w5tAV9XYy+z3X+il7EzpPoH2kNoL6M+wtzA4x9tYsqTX1AVCIZ9HhGJrf8s+I7HP9pY77Fcijnd/CLyzqs88NWsyNuLSKvVpm4X9u/UeBCyg6kEK7viTN6/OD6/npCyyurB08HTeS04otatv77GViY53ucCx0ekGg3PfmUbZbzv/hXb7By+sA9jZmAUX9qHNjl+O8RMm0b1LcWF6/Zw66wVPHTpkSG1E5HY2FG2g9nrZvNlwZcUV/j46otxNPTj8jBzC24jul+G1i7/iH5HXdl0RRFpU9pUyDokN5Vje2ezeEMji9/N2uHIdcAOFSV2Mjf5b2SBNaTetuvsbkwNXMm9gfH82fUEFzoaXsTqMGx6GHvowR4udHzESqsnd/qvYKndr8E2mTS+puwHFQfs0fXGVzu58vhChvXIDqmtiERuQ/EG/m/p//HR9o+w7Orb9b59J2BZDf+oTCb65Qjrtuwku7SK3DStxRRpT9rU7UKAy47t2eAxM2kzjsTar6spcVR/xRI7mXG+qQ0GrAOVk8St/uuYEWj6icAfHG5u4hn33xhlLq33uJMAP3HUv7/Xj63/flPSHzzVyLYVIhIby3YvY9Lbk1iwbUFNwALwFR/TaDtvIxsXh6rUSuCFz7dE3Y+ItC5tLmSdMbAjw3vWv0mnK2NZnbJPkpIAuN5/M2vsxveq+rG/BC5jXvCIkOsnGn4edD3IEGN9nWOjzGXkG0VN9mHZBs8HT6lV9vbKXewrq7vYVkRiY33Rem54/wZKfbVnm+1gArYvt8F2hmsvW7LXUN/NQhsocDjY4HKy3emgqpFtWb61u/L4RxtrvYdVRNq+NnW7EMDpMHn88qOY8PgSvtnpqXXMdBXXqf9xUiJvGv35yurNZMc7XOxYQBdjL24CeEjmE2sQTwdOq9lp/UAWJg8FzuMUx4qQx5dk+LjD9RzjfX+oVX65472Q2n9sDWLTAe8yBPAFLdbuKuW4PrqVIBJrhZWF3DzvZsoCZXWO2Vb9/80ZjlISO72II3Ut+wybT4KJnPT9U8zFpsnLaSnMSktlm8tV0ybFshhTVs44TxmH+vfv6ee13bwYPBGP18/aXR4GdA7tARkRaf3a3EwWQGaym9nXjuDcIZ1xHLhrZz2LT02gwk5mccIN3Ol6ioHmZjKNcpKNKvKNIi50fMQrCVN5zf07Bhp1nx5aavdrcrf2HzvW/IY+xraazz9zvMZxjtVNtvPZDv4dOL/eY6VVjW9fISLh+9/K//GT2T9hc2n9t+QNs+6bFwxnMck9H8WZtqbmZfTPp6cBMDM9jVHdOvN/2Vm1AhZAuWnyQnoaF3TtxC15OVR8/7PrteBxeEgF4K2V4T+1LCKtV5sMWQCpCU4eGD+Uj359CumJTpKpxAy6a9Vx2jYP7CrkYpaSYjR+u22wuZFZ7j8zwvy6zrFHA2Ox7PC2YJ/keB+AaxxvcIfz+SbrB2yTX/t/xhf2YfUeT3Y76i0Xkcj84/N/8MCyBwhYjfwCY1ZiOEv2fzaqSOo+HdNd++Gbj5MS+V1ONvd2yKLKbPrH6tyUZH6an0cByTwWHFtT/t7q3Y20EpG2ps2GrB90zkriL0NL+DLhGv5cVftpwDv3FnJyZeib/KUYVfzH9X8cesAsFMDr1vGcWHU/DwfOYZ+dFlJfgx3rcOPjcHMDNo0HtD12Btf4b+EV64R6j5sG9M5NDe1LiEiT5qybw5Orn2yynmGAK/Ozms+uzC9wJBTUfLZtA9tyYRsGr6WF99/oV4kJnJ9zJBsPWB6wYU/dW5Yi0na1+ZAFMGbXIyQYAc4qryDt+13S+/p8nFsW/i7K6YaXXzhfrFO+nVzuC4xnRNWDzAqc3HQ/7u1YaWv4uf8mRvr+j/8GzqbIrv1DeIl1GD/33ciIqgf50Gp4L6yR/fLokpkU9ncRkbos2+K/X/035PrVIStIB0rIz34PK5BK1d5TKVv/a8rW3E3Z2r8QrMqJaCyetC2Y7v2hLWBBpV8bEIu0F21u4Xsd25fh2Fm9NUKSbXNuWRkzM9IZVxL5b4Snm1+QSxF7qPsUow83vw78DC8JXOFseDF7hWmQkPMOgdJBbLE78rfARP4WmEgKXhLw4yGZQIj/+i87tkfE30VEaluwdQE7y0Nf+2S6Sumf+QbXW/O5pfBi/MVHg73/v11H8nocCXsjHs+knL8xaa+Lt6xjmGX9hESXlgaItBdtfybri2m1Pl5XXEKXvf05u7TugtVQuY0g4x3zGq3zp8DlLAoOqPdYAHg9JQXTXUhi16dx576HM20lEKScJApJDzlgHd0zm5P7NvwIuYiE56VvXwqrfhd/gOlVL/HvyqvwFx3HQLZxj/O/LHDfzPKEn3J2h0ejGs/cNDe9zO3c7HyJhe6bYM4U8Da93YuItH7tYCar9gaf/6u6AIf3SFIS3o2q2wHmZurd/OZ7Fib/CY6p9dRgkWnybHoaL6alsMdZ/a/WlfYNpH1T3cafhr94OP6i47CDTa/fOCw/jf9ePgzTDG/RvYg0rKEnCRvy5737+JdvMm4rlZfdUxlq1t4Hr8AV3XrJMtOk0OGgYzCIkyCsehF2rYIrXoe0jlH1LSItq+3PZB2weeCTgdP5d/BCUmPwmouUEPpYaA1mi5UHwAaXk/Gd83ksK6MmYP2Y6SolIfdDkns9hOlu+Ckiw6jedHXWtSPITHY3WE9EwlcVCH1j3z4+H7kVWWyzc3jB/ec6AQugyoj+l6A6fexdC89eDH5v1H2LSMtp+yHLlQJAqZ3EvYHxAJTH4DUXFQe8OLohNiZzgiexw+ng6vw8drhCmxg0XcUk9Xgcw1WI44BZquwUN1ef0It5t47kP5cdRXqiq5FeRCQSqe7QZ57Gecp4N3gUj7nub/Cl8akx2KU91bLqFu5cASuei7pvEWk5bf92YW4/2PMNLwZPrAlGm+18vLabJCPydVlr7W4h1dtm5/CHnA4Nzl41xHSWkdhpNnMnPE9mshunaWjBq0gzODLvSNYVrQup7vHlfr41t5FuNDyjNKyykuWJkf9i18vnJ7u+kAXw+XQ4akrEfYtIy2r7M1lHXQnAM8FRNUWlJPN6cETEXQZsk+cCpzRdESh0e/ksqelZr/o4UzbiZTupCU4FLJFm4AtYfPvd4SHXDwaTOcn8qtE6F5eWYdqRz2aNKy1t+ODulbBlScR9i0jLavshq/dISrMP51u7a63ip4KnRdzlB9aR7KJDSHW3ZdddoxGOF9a+EFV7EQmNbdvcNmcF81c6CJT3CqlNIgGcRgOzTN/rEghygjeydaBJlsU5pU3s57dzeUR9i0jLa/shCyg79ld1ylbZvXkjeEzYfVXYCTwQuCDk+rvSonvX2AdbPoiqvYiE5p1Vu3h1+Q4AqnaPxQ42/VBJYhOv4/rBrYVFpAfD30T0V4XFpDU1C1bVyEyXiLRq7SJkJQ06q97yW/3XNbiXVX0qbRc3+G9itd0zpPqGswSM6F7cXFxVHFV7EQnNU5/u37rBquqMd9vl2MHG11JVOEILTr39AR7evSesoPWLwmIuKQ1h0+QwFuqLSOvSLkJWeqKLnNS6PyyrcHOF/w5mBn5Cld34wvTvrE5M9P2WedbQkM/ryvycaJ/eNtvHX4FIq7a+oIxPN+yrVRas6EPF5msJlPWlocmkrSE+MQwwpMrH0zt3M7K8Akcjs1N9q3z8c/ceri7xhNZxXv0vjReR1q/tP10ImKbBuOFdeXjed3WO+XHy+8BV/CtwEeMc87jYsYAuxl7cRhCPncQn1iCeDp7GImtQeOdM3Ia7w8Kox56TFNk7z0QkdAvW7am33KrqhHfrFFyZn5LY6dU6x1cnuDmmMvR9tXr7AzxYsJedDgez01NZmpBIuWmQaNv08Pu5sLScI6tC74/sQ6BX0+9KFZHWqV2ELIBLj+nBYws2EGxgz5p9ZPBI8DweCZ4HgImFFekskllBQs77YJvYNlHNZo3uNTryxiISkhKvv9Hj/uJjcGYsx5lcezf4OWmpXFFSGvZPik7BIDcVlbDPtuhgRLGm6qgp0f2AEZEW1W7uVXXJTGLc8ND2tgIiD1gAVjLebZMpW3cnvqKjI+7GNEwu6XdJ5OMQkZC4mnw1lYl32+UEK/NrlW5xuVicGNkWLUBNwIpoh4esXnDkZRGfW0RaXrsJWQB/Omcgp/RrzpcpB6vfTRihU7udSufUzjEcj4jUp1NmUtOVgilUbL6WbqUdau179UhWBpFsa/xmcBjDA3dzrHkr52SeQIEjjL3w0jrDpBchMSOCM4tIa9GuQpbLYfL45Ucx6djuOJvhpcrOtNWYrshuBfTO6M2dx90Z2wGJSL3OHJRPijuEkGMlMmznEby1bQdTikvICwRYmeDmt7kdCOc54q1OB1N7F1B56KOU95jNpuwtTOzckWUJIewM3/04uPp96HBIGGcUkdbIsO0otiqOkMfjISMjg5KSEtLT0+Nyjl0llTy7ZDPPf76VgtIwFpqGIan74zhT6i62b8rhOYfz4KkP0iEptA1PRSR6v39lJTMXb2my3gnmSma67675bAMWEM47GW7L7cA7qSn1Hutf5eMSTymnl1eSbn+/5UNiBgw4D4ZfDZ0Gh3EmkebVHNfv9qTdhqwf2LZNWVUAry9IVcDi1eXbWbXdQ1lVALfLpjCwmlVbIVDZMax+nenLSOoyK6w2R+Qewbh+4ziz55m4HHr5s0hzWl9Qxtn//oiqQOM7uBtYLHD/ku5m/U8kNmWvw+S0bl0INLFg/ajkG5hx/vfvJXQ2vTGqSGugkBWedvN0YUMMwyAt0UVaYnWoufHUQ39U41i2FBUz7vGP2FkY2t1TZ9oqEjvPafD4sZ2O5erDr6agooByfzkprhT6ZvWlX3a/SL+GiESpT14q/xo3hBufXUYDDyEDYGPybPAn3GE+H9F5XkpNbTJgVRWczm8vn6hwJdLOtfuZrFCVVQX4x7treXHpNkqr6l99ke/wcHnmSo4dWMGsBIsPdy3GG/ACkO5OZ3Sv0YzvN54+WX2ac+giEoa5q3dz03Nf4vU3vDt7BmW8m3A7+UZRWH3vM00u7pLPHmf9v7/awUSqCs7i6iETuP1MbTIqbU9rvH63ZgpZP1JeFeDlL7czd/Vuiit8mKZBXloC5w3pwmkDOuJ01J7tqgpWYWLq9p9IG1JY7mPWF1t5ZslmthZ6ax3rZexkouN9xjnmk2Z46++gHqV2EpcHb+br9EJcWUsw3YU1x4LeLviLjsXvOYLLjjmUP587EEP7X0kb1Jqv362RQpaIHLQsy2bVjhL27d6OsXIWOZvfZqC1Nuz9P63MXvy30508/E0SpZU/zIQHwfSBlQCY9MlL5ZoTe3NJGPv5ibQ2un6HRyFLROQH3iJY/ixs/QyqPOBKhg59ILcfrHkT1r4NPzwRaJjQZ1T1E4F9TgPTxOsL8sry7cxfW0CJ14/LYdIxPZELjuzCcYfoFVrS9un6HR6FLBGRUPkqqoMYNiRlgbv+bRpE2itdv8PT7p8uFBGJGXdy9T8iIiFoVzu+i4iIiLQWClkiIiIicaCQJSIiIhIHClkiIiIicaCQJSIiIhIHLfJ04Q+7Rng8npY4vYiIiETgh+t2C+z+1Ca1SMgqLS0FoFs37XwsIiLS1pSWlpKRkdHSw2j1WmQzUsuy2LFjB2lpaXp/l4iISBth2zalpaV07twZ09SKo6a0SMgSERERae8UQ0VERETiQCFLREREJA4UskRERETiQCFLREREJA4UskQOQiNHjuTmm2+uU/7EE0+QmZkJwJ133olhGJx55pl16t13330YhsHIkSPrHNu2bRtut5tBgwbVe27DMGr+ycjI4Pjjj+fDDz+sOb5w4ULGjh1L586dMQyDV155JZKvKCLS4hSyRKRBnTp1Yt68eWzbtq1W+fTp0+nevXu9bZ544gkuueQSPB4PS5YsqbfOjBkz2LlzJ5988gk5OTmMGTOGDRs2AFBeXs4RRxzBww8/HNsvIyLSzBSyRKRBeXl5nH766Tz55JM1ZYsWLWLv3r2cffbZderbts2MGTO47LLLuPTSS5k2bVq9/WZmZpKfn8+gQYN49NFH8Xq9zJ07F4DRo0dz1113cf7558fnS4mINBOFLBFp1JQpU3jiiSdqPk+fPp2JEyfidrvr1J03bx4VFRWMGjWKSZMm8fzzz1NeXt5o/0lJSQD4fL6YjltEpKUpZIlIo8aMGYPH42HhwoWUl5cza9YspkyZUm/dadOmMX78eBwOB4MGDaJ3797Mnj27wb4rKir4/e9/j8Ph4OSTT47XVxARaREt8u5CEWk7XC4XkyZNYsaMGWzYsIG+ffsyePDgOvWKi4t56aWX+Pjjj2vKJk2axLRp05g8eXKtuhMmTMDhcOD1esnNzWXatGn19iki0pYpZIkchNLT0ykpKalTXlxcXO9LX6dMmcIxxxzDqlWrGpzFevbZZ6msrOSYY46pKbNtG8uyWLduHX379q0p/9e//sWoUaPIyMggNzc3Bt9IRKT10e1CkYNQv379WLZsWZ3yZcuW1QpDPxg4cCADBw5k1apVXHrppfX2OW3aNG699VaWL19e88+KFSs48cQTmT59eq26+fn59OnTRwFLRNo1zWSJHISuu+46HnroIW666SauvvpqEhISePPNN3nuued4/fXX623z4Ycf4vf7a/bROtDy5ctZtmwZzzzzDIcddlitYxMmTODPf/4zd911F05n0z9yysrKWL9+fc3njRs3snz5crKzsxvcNkJEpDXSTJbIQah3794sXLiQNWvWMGrUKI455hhmzZrF7Nmz6918FCAlJaXegAXVs1gDBgyoE7AAzj//fAoKCnjrrbdCGtsXX3zB0KFDGTp0KAC33HILQ4cOZerUqaF9ORGRVsKwbdtu6UGIiIiItDeayRIRERGJA4UsERERkThQyBIRERGJA4UsERERkThQyBIRERGJA4UsERERkThQyBIRERGJA4UsERERkThQyBIRERGJA4UsERERkThQyBIRERGJA4UsERERkTj4f+8XthVm9diGAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wang3712/.local/lib/python3.8/site-packages/scanpy/plotting/_tools/scatterplots.py:392: UserWarning: No data for colormapping provided via 'c'. Parameters 'cmap' will be ignored\n",
      "  cax = scatter(\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sc.tl.pca(adata, svd_solver=\"arpack\")\n",
    "sc.pp.neighbors(adata, n_neighbors=10)\n",
    "sc.tl.umap(adata)\n",
    "sc.pl.umap(adata, color='tumor')\n",
    "sc.pl.umap(adata, color='metas_site')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b405670-a0df-49a3-83a7-51e1d97cf96b",
   "metadata": {},
   "source": [
    "## Data normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0c6828-5e9e-4354-91f4-429f0779e3dc",
   "metadata": {},
   "source": [
    "**Data normalzation** is strongly recommended for training PreMet on new data. We observed similar perofrmance using either min-max or z-score normalization. \n",
    "\n",
    "Without normalization, loss functino need to be modified as ZINB loss for better training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c18517ea-4ae2-41bc-a4fc-a1eb8dc30da0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gene_1</th>\n",
       "      <th>Gene_2</th>\n",
       "      <th>Gene_3</th>\n",
       "      <th>Gene_4</th>\n",
       "      <th>Gene_5</th>\n",
       "      <th>Gene_6</th>\n",
       "      <th>Gene_7</th>\n",
       "      <th>Gene_8</th>\n",
       "      <th>Gene_9</th>\n",
       "      <th>Gene_10</th>\n",
       "      <th>...</th>\n",
       "      <th>Gene_991</th>\n",
       "      <th>Gene_992</th>\n",
       "      <th>Gene_993</th>\n",
       "      <th>Gene_994</th>\n",
       "      <th>Gene_995</th>\n",
       "      <th>Gene_996</th>\n",
       "      <th>Gene_997</th>\n",
       "      <th>Gene_998</th>\n",
       "      <th>Gene_999</th>\n",
       "      <th>Gene_1000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Sample_1</th>\n",
       "      <td>0.631304</td>\n",
       "      <td>0.253464</td>\n",
       "      <td>0.236414</td>\n",
       "      <td>0.157031</td>\n",
       "      <td>0.187718</td>\n",
       "      <td>0.206734</td>\n",
       "      <td>0.366497</td>\n",
       "      <td>0.188599</td>\n",
       "      <td>0.453102</td>\n",
       "      <td>0.382513</td>\n",
       "      <td>...</td>\n",
       "      <td>0.130114</td>\n",
       "      <td>0.218273</td>\n",
       "      <td>0.299943</td>\n",
       "      <td>0.219280</td>\n",
       "      <td>0.275779</td>\n",
       "      <td>0.224045</td>\n",
       "      <td>0.103147</td>\n",
       "      <td>0.343903</td>\n",
       "      <td>0.467347</td>\n",
       "      <td>0.066796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sample_2</th>\n",
       "      <td>0.421580</td>\n",
       "      <td>0.310753</td>\n",
       "      <td>0.190961</td>\n",
       "      <td>0.129297</td>\n",
       "      <td>0.228260</td>\n",
       "      <td>0.376873</td>\n",
       "      <td>0.109995</td>\n",
       "      <td>0.096386</td>\n",
       "      <td>0.100375</td>\n",
       "      <td>0.157855</td>\n",
       "      <td>...</td>\n",
       "      <td>0.282597</td>\n",
       "      <td>0.102349</td>\n",
       "      <td>0.135234</td>\n",
       "      <td>0.427315</td>\n",
       "      <td>0.282761</td>\n",
       "      <td>0.236014</td>\n",
       "      <td>0.326282</td>\n",
       "      <td>0.214574</td>\n",
       "      <td>0.312433</td>\n",
       "      <td>0.367586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sample_3</th>\n",
       "      <td>0.103795</td>\n",
       "      <td>0.082508</td>\n",
       "      <td>0.184824</td>\n",
       "      <td>0.424682</td>\n",
       "      <td>0.472873</td>\n",
       "      <td>0.159543</td>\n",
       "      <td>0.096039</td>\n",
       "      <td>0.081114</td>\n",
       "      <td>0.085489</td>\n",
       "      <td>0.202106</td>\n",
       "      <td>...</td>\n",
       "      <td>0.094276</td>\n",
       "      <td>0.332807</td>\n",
       "      <td>0.066102</td>\n",
       "      <td>0.375563</td>\n",
       "      <td>0.218357</td>\n",
       "      <td>0.054407</td>\n",
       "      <td>0.333216</td>\n",
       "      <td>0.013715</td>\n",
       "      <td>0.191011</td>\n",
       "      <td>0.096702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sample_4</th>\n",
       "      <td>0.300698</td>\n",
       "      <td>0.226939</td>\n",
       "      <td>0.289379</td>\n",
       "      <td>0.142094</td>\n",
       "      <td>0.254186</td>\n",
       "      <td>0.244997</td>\n",
       "      <td>0.120232</td>\n",
       "      <td>0.104817</td>\n",
       "      <td>0.402590</td>\n",
       "      <td>0.063747</td>\n",
       "      <td>...</td>\n",
       "      <td>0.315731</td>\n",
       "      <td>0.111571</td>\n",
       "      <td>0.386850</td>\n",
       "      <td>0.196863</td>\n",
       "      <td>0.385262</td>\n",
       "      <td>0.386793</td>\n",
       "      <td>0.162316</td>\n",
       "      <td>0.442168</td>\n",
       "      <td>0.218328</td>\n",
       "      <td>0.062701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sample_5</th>\n",
       "      <td>0.050774</td>\n",
       "      <td>0.120429</td>\n",
       "      <td>0.318987</td>\n",
       "      <td>0.090415</td>\n",
       "      <td>0.557622</td>\n",
       "      <td>0.270865</td>\n",
       "      <td>0.135583</td>\n",
       "      <td>0.189404</td>\n",
       "      <td>0.123768</td>\n",
       "      <td>0.494423</td>\n",
       "      <td>...</td>\n",
       "      <td>0.240585</td>\n",
       "      <td>0.126192</td>\n",
       "      <td>0.295626</td>\n",
       "      <td>0.371984</td>\n",
       "      <td>0.197384</td>\n",
       "      <td>0.088957</td>\n",
       "      <td>0.034549</td>\n",
       "      <td>0.190474</td>\n",
       "      <td>0.313075</td>\n",
       "      <td>0.199449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sample_196</th>\n",
       "      <td>0.227064</td>\n",
       "      <td>0.036469</td>\n",
       "      <td>0.486900</td>\n",
       "      <td>0.217048</td>\n",
       "      <td>0.271973</td>\n",
       "      <td>0.194673</td>\n",
       "      <td>0.275914</td>\n",
       "      <td>0.617982</td>\n",
       "      <td>0.373461</td>\n",
       "      <td>0.432137</td>\n",
       "      <td>...</td>\n",
       "      <td>0.174224</td>\n",
       "      <td>0.206152</td>\n",
       "      <td>0.287405</td>\n",
       "      <td>0.367035</td>\n",
       "      <td>0.106542</td>\n",
       "      <td>0.141887</td>\n",
       "      <td>0.321040</td>\n",
       "      <td>0.791322</td>\n",
       "      <td>0.305602</td>\n",
       "      <td>0.187106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sample_197</th>\n",
       "      <td>0.127077</td>\n",
       "      <td>0.236917</td>\n",
       "      <td>0.306406</td>\n",
       "      <td>0.142492</td>\n",
       "      <td>0.172541</td>\n",
       "      <td>0.125308</td>\n",
       "      <td>0.118162</td>\n",
       "      <td>0.245801</td>\n",
       "      <td>0.106035</td>\n",
       "      <td>0.055299</td>\n",
       "      <td>...</td>\n",
       "      <td>0.665133</td>\n",
       "      <td>0.390331</td>\n",
       "      <td>0.282429</td>\n",
       "      <td>0.046088</td>\n",
       "      <td>0.181594</td>\n",
       "      <td>0.621529</td>\n",
       "      <td>0.390801</td>\n",
       "      <td>0.476447</td>\n",
       "      <td>0.081324</td>\n",
       "      <td>0.054135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sample_198</th>\n",
       "      <td>0.377015</td>\n",
       "      <td>0.160854</td>\n",
       "      <td>0.357289</td>\n",
       "      <td>0.199131</td>\n",
       "      <td>0.163302</td>\n",
       "      <td>0.295824</td>\n",
       "      <td>0.385046</td>\n",
       "      <td>0.164063</td>\n",
       "      <td>0.277299</td>\n",
       "      <td>0.168548</td>\n",
       "      <td>...</td>\n",
       "      <td>0.161975</td>\n",
       "      <td>0.106867</td>\n",
       "      <td>0.668855</td>\n",
       "      <td>0.467984</td>\n",
       "      <td>0.443416</td>\n",
       "      <td>0.133918</td>\n",
       "      <td>0.090300</td>\n",
       "      <td>0.364668</td>\n",
       "      <td>0.211606</td>\n",
       "      <td>0.401612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sample_199</th>\n",
       "      <td>0.308279</td>\n",
       "      <td>0.485271</td>\n",
       "      <td>0.296326</td>\n",
       "      <td>0.140796</td>\n",
       "      <td>0.079451</td>\n",
       "      <td>0.124490</td>\n",
       "      <td>0.262844</td>\n",
       "      <td>0.238820</td>\n",
       "      <td>0.044268</td>\n",
       "      <td>0.116508</td>\n",
       "      <td>...</td>\n",
       "      <td>0.532522</td>\n",
       "      <td>0.375959</td>\n",
       "      <td>0.524934</td>\n",
       "      <td>0.123977</td>\n",
       "      <td>0.031445</td>\n",
       "      <td>0.791469</td>\n",
       "      <td>0.233568</td>\n",
       "      <td>0.099543</td>\n",
       "      <td>0.290569</td>\n",
       "      <td>0.056958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sample_200</th>\n",
       "      <td>0.208576</td>\n",
       "      <td>0.468387</td>\n",
       "      <td>0.205984</td>\n",
       "      <td>0.264524</td>\n",
       "      <td>0.249227</td>\n",
       "      <td>0.118691</td>\n",
       "      <td>0.112119</td>\n",
       "      <td>0.162925</td>\n",
       "      <td>0.160998</td>\n",
       "      <td>0.224255</td>\n",
       "      <td>...</td>\n",
       "      <td>0.110256</td>\n",
       "      <td>0.708006</td>\n",
       "      <td>0.202287</td>\n",
       "      <td>0.407636</td>\n",
       "      <td>0.312412</td>\n",
       "      <td>0.068105</td>\n",
       "      <td>0.362866</td>\n",
       "      <td>0.233360</td>\n",
       "      <td>0.011096</td>\n",
       "      <td>0.470339</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Gene_1    Gene_2    Gene_3    Gene_4    Gene_5    Gene_6  \\\n",
       "Sample_1    0.631304  0.253464  0.236414  0.157031  0.187718  0.206734   \n",
       "Sample_2    0.421580  0.310753  0.190961  0.129297  0.228260  0.376873   \n",
       "Sample_3    0.103795  0.082508  0.184824  0.424682  0.472873  0.159543   \n",
       "Sample_4    0.300698  0.226939  0.289379  0.142094  0.254186  0.244997   \n",
       "Sample_5    0.050774  0.120429  0.318987  0.090415  0.557622  0.270865   \n",
       "...              ...       ...       ...       ...       ...       ...   \n",
       "Sample_196  0.227064  0.036469  0.486900  0.217048  0.271973  0.194673   \n",
       "Sample_197  0.127077  0.236917  0.306406  0.142492  0.172541  0.125308   \n",
       "Sample_198  0.377015  0.160854  0.357289  0.199131  0.163302  0.295824   \n",
       "Sample_199  0.308279  0.485271  0.296326  0.140796  0.079451  0.124490   \n",
       "Sample_200  0.208576  0.468387  0.205984  0.264524  0.249227  0.118691   \n",
       "\n",
       "              Gene_7    Gene_8    Gene_9   Gene_10  ...  Gene_991  Gene_992  \\\n",
       "Sample_1    0.366497  0.188599  0.453102  0.382513  ...  0.130114  0.218273   \n",
       "Sample_2    0.109995  0.096386  0.100375  0.157855  ...  0.282597  0.102349   \n",
       "Sample_3    0.096039  0.081114  0.085489  0.202106  ...  0.094276  0.332807   \n",
       "Sample_4    0.120232  0.104817  0.402590  0.063747  ...  0.315731  0.111571   \n",
       "Sample_5    0.135583  0.189404  0.123768  0.494423  ...  0.240585  0.126192   \n",
       "...              ...       ...       ...       ...  ...       ...       ...   \n",
       "Sample_196  0.275914  0.617982  0.373461  0.432137  ...  0.174224  0.206152   \n",
       "Sample_197  0.118162  0.245801  0.106035  0.055299  ...  0.665133  0.390331   \n",
       "Sample_198  0.385046  0.164063  0.277299  0.168548  ...  0.161975  0.106867   \n",
       "Sample_199  0.262844  0.238820  0.044268  0.116508  ...  0.532522  0.375959   \n",
       "Sample_200  0.112119  0.162925  0.160998  0.224255  ...  0.110256  0.708006   \n",
       "\n",
       "            Gene_993  Gene_994  Gene_995  Gene_996  Gene_997  Gene_998  \\\n",
       "Sample_1    0.299943  0.219280  0.275779  0.224045  0.103147  0.343903   \n",
       "Sample_2    0.135234  0.427315  0.282761  0.236014  0.326282  0.214574   \n",
       "Sample_3    0.066102  0.375563  0.218357  0.054407  0.333216  0.013715   \n",
       "Sample_4    0.386850  0.196863  0.385262  0.386793  0.162316  0.442168   \n",
       "Sample_5    0.295626  0.371984  0.197384  0.088957  0.034549  0.190474   \n",
       "...              ...       ...       ...       ...       ...       ...   \n",
       "Sample_196  0.287405  0.367035  0.106542  0.141887  0.321040  0.791322   \n",
       "Sample_197  0.282429  0.046088  0.181594  0.621529  0.390801  0.476447   \n",
       "Sample_198  0.668855  0.467984  0.443416  0.133918  0.090300  0.364668   \n",
       "Sample_199  0.524934  0.123977  0.031445  0.791469  0.233568  0.099543   \n",
       "Sample_200  0.202287  0.407636  0.312412  0.068105  0.362866  0.233360   \n",
       "\n",
       "            Gene_999  Gene_1000  \n",
       "Sample_1    0.467347   0.066796  \n",
       "Sample_2    0.312433   0.367586  \n",
       "Sample_3    0.191011   0.096702  \n",
       "Sample_4    0.218328   0.062701  \n",
       "Sample_5    0.313075   0.199449  \n",
       "...              ...        ...  \n",
       "Sample_196  0.305602   0.187106  \n",
       "Sample_197  0.081324   0.054135  \n",
       "Sample_198  0.211606   0.401612  \n",
       "Sample_199  0.290569   0.056958  \n",
       "Sample_200  0.011096   0.470339  \n",
       "\n",
       "[200 rows x 1000 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# here, we use min max norm\n",
    "df_normalized = adata.to_df().apply(lambda x: (x - x.min()) / (x.max() - x.min()), axis=1)\n",
    "df_normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c745b6-5256-476a-84c7-10987c9b586a",
   "metadata": {},
   "source": [
    "# One hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "97deccff-acfa-4adf-9289-3e4dc724d77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encoding the labels\n",
    "def one_hot_encode(labels, num_classes):\n",
    "    unique_labels = list(set(labels))\n",
    "    \n",
    "    one_hot_encoded = np.zeros(shape=(len(labels), num_classes))\n",
    "\n",
    "    # Encode labels by setting the corresponding index to 1 in each row\n",
    "    for i, label in enumerate(labels):\n",
    "        index = unique_labels.index(label)\n",
    "        one_hot_encoded[i, index] = 1\n",
    "\n",
    "    return one_hot_encoded, unique_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c8a025e1-7cbc-4d49-9548-f02bf9e7f34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_labels_tissue,_ = one_hot_encode(adata.obs['tumor'], len(adata.obs['tumor'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c2025992-e2a1-46b9-a632-3a7bfe9d09c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_labels_site, uniqe_labels_site = one_hot_encode(adata.obs['metas_site'], len(adata.obs['metas_site'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0b771e3b-ad5a-4075-b681-b5c49add39e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "one_hot_labels_site_one_vs_all = dict()\n",
    "all_sites = adata.obs.metas_site.value_counts().index\n",
    "for tissue in all_sites:\n",
    "    print(tissue)\n",
    "    tissues_to_learn = tissue\n",
    "    one_vs_all_labels = []\n",
    "    for i in adata.obs.metas_site:\n",
    "        if  i == tissues_to_learn:\n",
    "            one_vs_all_labels.append(1)\n",
    "        else:\n",
    "            one_vs_all_labels.append(0)\n",
    "    adata.obs[tissue+'.1va'] = one_vs_all_labels\n",
    "    one_hot_labels_site_one_vs_all[tissue], _ = one_hot_encode(adata.obs[tissue+'.1va'], len(adata.obs[tissue+'.1va'].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c99e3b-bf10-406f-b119-f6024efde638",
   "metadata": {},
   "source": [
    "# DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90f262ec-a3a7-4e7e-8050-de19955d41ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd9daa5e-196b-4234-83e6-46dbcc15fb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneExpressionDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, tumors, metas_sites):\n",
    "        self.data = data  # gene expression data (shape: N x num_genes)\n",
    "        self.tumors = tumors  # tiprimary tumor type\n",
    "        self.site = metas_sites # metastasis sites\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        expression = self.data[idx]  # single gene expression vector\n",
    "        tumor = self.tumors[idx]  \n",
    "        site = self.site[idx]\n",
    "        return expression, tumor, site"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9607a61f-0e24-4f05-9d39-ab524594bc63",
   "metadata": {},
   "source": [
    "# Cross-validation set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d46b5033-0dfd-48f8-8c35-33f16c956717",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = GeneExpressionDataset(torch.tensor(df_normalized.to_numpy(), dtype=torch.float32), \n",
    "                                 torch.tensor(one_hot_labels_tissue, dtype=torch.float32),\n",
    "                                 torch.tensor(one_hot_labels_site, dtype=torch.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7a907348-a3f8-4fd7-9972-e40c00a02b93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Validation Tissue Labels: (array([0, 1, 2]), array([15, 15, 10])) 3\n",
      "Validation Site Labels: (array([0, 1]), array([21, 19])) 2\n",
      "Fold 2\n",
      "Validation Tissue Labels: (array([0, 1, 2]), array([ 7, 15, 18])) 3\n",
      "Validation Site Labels: (array([0, 1]), array([21, 19])) 2\n",
      "Fold 3\n",
      "Validation Tissue Labels: (array([0, 1, 2]), array([16, 13, 11])) 3\n",
      "Validation Site Labels: (array([0, 1]), array([21, 19])) 2\n",
      "Fold 4\n",
      "Validation Tissue Labels: (array([0, 1, 2]), array([ 9, 16, 15])) 3\n",
      "Validation Site Labels: (array([0, 1]), array([21, 19])) 2\n",
      "Fold 5\n",
      "Validation Tissue Labels: (array([0, 1, 2]), array([16, 15,  9])) 3\n",
      "Validation Site Labels: (array([0, 1]), array([21, 19])) 2\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "# import numpy as np\n",
    "\n",
    "\n",
    "# Convert to class indices\n",
    "tissue_labels = np.argmax(one_hot_labels_tissue, axis=1)  # Convert one-hot to class indices\n",
    "site_labels = np.argmax(one_hot_labels_site, axis=1)  # Convert one-hot to class indices\n",
    "\n",
    "# Prepare StratifiedKFold\n",
    "n_splits = 5\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)  # Use a fixed random_state for repeatability\n",
    "\n",
    "# Store seeds and indices\n",
    "cv_splits = []\n",
    "\n",
    "batch_size = 8\n",
    "\n",
    "# Iterate through the splits\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(df_normalized.to_numpy(), site_labels)):\n",
    "    # print(train_idx)\n",
    "    # print(val_idx)\n",
    "    print(f'Fold {fold+1}')\n",
    "    \n",
    "    # Extract the training and validation data based on the indices\n",
    "    train_data = torch.utils.data.Subset(all_data, train_idx)\n",
    "    val_data = torch.utils.data.Subset(all_data, val_idx)\n",
    "\n",
    "    train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_data, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    cv_splits.append({'train_idx': train_idx, 'val_idx': val_idx})\n",
    "    \n",
    "    # Check stratification by printing unique labels in the validation set\n",
    "    val_tissue_labels = np.argmax(one_hot_labels_tissue[val_idx], axis=1)\n",
    "    val_site_labels = np.argmax(one_hot_labels_site[val_idx], axis=1)\n",
    "    \n",
    "    print('Validation Tissue Labels:', np.unique(val_tissue_labels, return_counts=True), len(np.unique(val_tissue_labels)))\n",
    "    print('Validation Site Labels:', np.unique(val_site_labels, return_counts=True), len(np.unique(val_site_labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804fd4f5-3581-4477-b2ee-a74c4442eeba",
   "metadata": {},
   "source": [
    "# Training and 5-fold CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b329d77a-58ec-4839-89e5-aa3af2cf9a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PreMet import PreMet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01ef6b9-c996-469d-880c-c24d3206ebfe",
   "metadata": {},
   "source": [
    "## Loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7eea62-ebbf-4ceb-92ff-0f5a4f5a0e77",
   "metadata": {},
   "source": [
    "The alpha and gamma in focal loss needs tuning if dataset is not balanced. \n",
    "\n",
    "mse_loss for recon_loss should be changed to ZINB loss, if data is not normalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "25377f62-3e96-458b-863f-06b16ae00e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=1.0, gamma=2.0, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        # Standard Cross Entropy Loss\n",
    "        ce_loss = nn.CrossEntropyLoss(reduction='none')(inputs, targets)\n",
    "\n",
    "        # Calculate pt (probability of true class)\n",
    "        pt = torch.exp(-ce_loss)\n",
    "\n",
    "        # Focal Loss \n",
    "        focal_loss = self.alpha * ((1 - pt) ** self.gamma) * ce_loss\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        else:\n",
    "            return focal_loss\n",
    "\n",
    "def mse_loss(y, recon_y):\n",
    "    return torch.mean((y - recon_y) ** 2)\n",
    "\n",
    "\n",
    "def loss_function_premet(y, recon_y, mean, logvar, \n",
    "                         tissue_labels, pred_tissue_labels, \n",
    "                         site_labels, pred_site_labels,\n",
    "                         alpha=1.0, beta=1.0, phi=1.0, delta = 1.0):\n",
    "    # VAE loss parts\n",
    "    recon_loss = mse_loss(y, recon_y)\n",
    "    kl_div = -0.5 * torch.sum(1 + logvar - mean.pow(2) - logvar.exp())\n",
    "\n",
    "    # add tumor type classification loss\n",
    "    tissue_loss = tissue_loss_func(tissue_labels, pred_tissue_labels)\n",
    "\n",
    "    # add metastasis site classification loss\n",
    "    site_loss = focal_loss(site_labels, pred_site_labels)\n",
    "\n",
    "    return alpha*recon_loss + phi*kl_div + beta*tissue_loss+delta*site_loss, beta*tissue_loss, delta*site_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1bd9b731-a73e-43b2-bf65-cb8354ea094b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize loss \n",
    "tissue_loss_func = nn.CrossEntropyLoss()\n",
    "# for metastasis site prediction, since it's balanced data, set gamma to 0\n",
    "focal_loss = FocalLoss(alpha=1.0, gamma=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c348188a-ab54-4611-9265-e44c67b51611",
   "metadata": {},
   "source": [
    "##  Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "16debef3-e142-4def-93b1-24e0460d6d98",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********** 1 **********\n",
      "++++++++++ 2 ++++++++++\n",
      "Val loss: 460.526648 | Val tissue acc 0.400000 | Val site acc 0.500000 | Val site auc 0.528822 | Val site f1 0.450549 | Val site auprc 0.647303  \n",
      "Epoch: 1/200 | Train Loss: 1130.899905 | Val loss: 434.044989 | Val tissue acc 0.325000 | Val site acc 0.500000 | Val site auc 0.601504 | Val site f1 0.498747 | Val site auprc 0.687281  \n",
      "Epoch: 51/200 | Train Loss: 62.362913 | Val loss: 128.899930 | Val tissue acc 0.275000 | Val site acc 0.500000 | Val site auc 0.523810 | Val site f1 0.498747 | Val site auprc 0.583718  \n",
      "Epoch: 101/200 | Train Loss: 57.082499 | Val loss: 134.288283 | Val tissue acc 0.225000 | Val site acc 0.325000 | Val site auc 0.343358 | Val site f1 0.321182 | Val site auprc 0.467381  \n",
      "Epoch: 151/200 | Train Loss: 57.885926 | Val loss: 121.037689 | Val tissue acc 0.375000 | Val site acc 0.500000 | Val site auc 0.453634 | Val site f1 0.498747 | Val site auprc 0.550603  \n",
      "############### Best prediction based on average\n",
      "accuracy 0.65\n",
      "f1_score 0.6500000000000001\n",
      "auc 0.6842105263157895\n",
      "auprc 0.7349233502079979\n",
      "++++++++++ 1 ++++++++++\n",
      "Val loss: 338.590692 | Val tissue acc 0.325000 | Val site acc 0.500000 | Val site auc 0.443609 | Val site f1 0.333333 | Val site auprc 0.432820  \n",
      "Epoch: 1/200 | Train Loss: 1066.773389 | Val loss: 385.142273 | Val tissue acc 0.400000 | Val site acc 0.450000 | Val site auc 0.533835 | Val site f1 0.448622 | Val site auprc 0.558810  \n",
      "Epoch: 51/200 | Train Loss: 68.370819 | Val loss: 125.961790 | Val tissue acc 0.350000 | Val site acc 0.450000 | Val site auc 0.408521 | Val site f1 0.444444 | Val site auprc 0.417903  \n",
      "Epoch: 101/200 | Train Loss: 58.074349 | Val loss: 130.498958 | Val tissue acc 0.250000 | Val site acc 0.500000 | Val site auc 0.378446 | Val site f1 0.430199 | Val site auprc 0.392484  \n",
      "Epoch: 151/200 | Train Loss: 57.855339 | Val loss: 122.546338 | Val tissue acc 0.325000 | Val site acc 0.400000 | Val site auc 0.395990 | Val site f1 0.340659 | Val site auprc 0.401997  \n",
      "############### Best prediction based on average\n",
      "accuracy 0.65\n",
      "f1_score 0.6419437340153452\n",
      "auc 0.5964912280701755\n",
      "auprc 0.5624010304431273\n",
      "********** 2 **********\n",
      "++++++++++ 2 ++++++++++\n",
      "Val loss: 415.960785 | Val tissue acc 0.175000 | Val site acc 0.475000 | Val site auc 0.416040 | Val site f1 0.322034 | Val site auprc 0.539904  \n",
      "Epoch: 1/200 | Train Loss: 1104.571561 | Val loss: 382.291846 | Val tissue acc 0.225000 | Val site acc 0.525000 | Val site auc 0.593985 | Val site f1 0.522313 | Val site auprc 0.569773  \n",
      "Epoch: 51/200 | Train Loss: 76.789692 | Val loss: 127.777739 | Val tissue acc 0.300000 | Val site acc 0.575000 | Val site auc 0.631579 | Val site f1 0.561573 | Val site auprc 0.694080  \n",
      "Epoch: 101/200 | Train Loss: 57.992286 | Val loss: 126.903363 | Val tissue acc 0.275000 | Val site acc 0.425000 | Val site auc 0.476190 | Val site f1 0.377958 | Val site auprc 0.486144  \n",
      "Epoch: 151/200 | Train Loss: 58.850527 | Val loss: 132.300314 | Val tissue acc 0.225000 | Val site acc 0.525000 | Val site auc 0.523810 | Val site f1 0.499671 | Val site auprc 0.516178  \n",
      "############### Best prediction based on average\n",
      "accuracy 0.7\n",
      "f1_score 0.6969696969696969\n",
      "auc 0.7293233082706767\n",
      "auprc 0.7497479666068978\n",
      "++++++++++ 1 ++++++++++\n",
      "Val loss: 295.389911 | Val tissue acc 0.375000 | Val site acc 0.500000 | Val site auc 0.416040 | Val site f1 0.466667 | Val site auprc 0.400051  \n",
      "Epoch: 1/200 | Train Loss: 1011.849710 | Val loss: 368.483832 | Val tissue acc 0.375000 | Val site acc 0.450000 | Val site auc 0.446115 | Val site f1 0.395604 | Val site auprc 0.505739  \n",
      "Epoch: 51/200 | Train Loss: 81.399802 | Val loss: 126.914630 | Val tissue acc 0.250000 | Val site acc 0.425000 | Val site auc 0.508772 | Val site f1 0.421747 | Val site auprc 0.505896  \n",
      "Epoch: 101/200 | Train Loss: 62.670254 | Val loss: 133.960240 | Val tissue acc 0.200000 | Val site acc 0.475000 | Val site auc 0.588972 | Val site f1 0.447005 | Val site auprc 0.643371  \n",
      "Epoch: 151/200 | Train Loss: 56.284281 | Val loss: 125.170877 | Val tissue acc 0.275000 | Val site acc 0.550000 | Val site auc 0.471178 | Val site f1 0.548872 | Val site auprc 0.470167  \n",
      "############### Best prediction based on average\n",
      "accuracy 0.6\n",
      "f1_score 0.6\n",
      "auc 0.6090225563909775\n",
      "auprc 0.6306782413631633\n",
      "********** 3 **********\n",
      "++++++++++ 2 ++++++++++\n",
      "Val loss: 341.348767 | Val tissue acc 0.325000 | Val site acc 0.500000 | Val site auc 0.721805 | Val site f1 0.373041 | Val site auprc 0.724252  \n",
      "Epoch: 1/200 | Train Loss: 1039.286313 | Val loss: 420.969958 | Val tissue acc 0.400000 | Val site acc 0.650000 | Val site auc 0.776942 | Val site f1 0.626667 | Val site auprc 0.741158  \n",
      "Epoch: 51/200 | Train Loss: 69.425342 | Val loss: 119.099380 | Val tissue acc 0.375000 | Val site acc 0.550000 | Val site auc 0.641604 | Val site f1 0.487179 | Val site auprc 0.607400  \n",
      "Epoch: 101/200 | Train Loss: 59.464729 | Val loss: 120.896507 | Val tissue acc 0.350000 | Val site acc 0.425000 | Val site auc 0.531328 | Val site f1 0.357093 | Val site auprc 0.550751  \n",
      "Epoch: 151/200 | Train Loss: 57.506422 | Val loss: 110.007568 | Val tissue acc 0.500000 | Val site acc 0.475000 | Val site auc 0.441103 | Val site f1 0.412998 | Val site auprc 0.541621  \n",
      "############### Best prediction based on average\n",
      "accuracy 0.7\n",
      "f1_score 0.6875\n",
      "auc 0.7593984962406015\n",
      "auprc 0.7542865720425642\n",
      "++++++++++ 1 ++++++++++\n",
      "Val loss: 381.641003 | Val tissue acc 0.300000 | Val site acc 0.525000 | Val site auc 0.476190 | Val site f1 0.344262 | Val site auprc 0.443222  \n",
      "Epoch: 1/200 | Train Loss: 1088.004907 | Val loss: 369.605579 | Val tissue acc 0.450000 | Val site acc 0.500000 | Val site auc 0.463659 | Val site f1 0.333333 | Val site auprc 0.440326  \n",
      "Epoch: 51/200 | Train Loss: 69.659034 | Val loss: 121.838120 | Val tissue acc 0.325000 | Val site acc 0.525000 | Val site auc 0.538847 | Val site f1 0.344262 | Val site auprc 0.540929  \n",
      "Epoch: 101/200 | Train Loss: 58.602515 | Val loss: 120.355063 | Val tissue acc 0.350000 | Val site acc 0.475000 | Val site auc 0.506266 | Val site f1 0.322034 | Val site auprc 0.500687  \n",
      "Epoch: 151/200 | Train Loss: 59.988363 | Val loss: 115.759154 | Val tissue acc 0.425000 | Val site acc 0.550000 | Val site auc 0.523810 | Val site f1 0.400000 | Val site auprc 0.540789  \n",
      "############### Best prediction based on average\n",
      "accuracy 0.5\n",
      "f1_score 0.3730407523510972\n",
      "auc 0.7192982456140351\n",
      "auprc 0.6763168573131397\n",
      "********** 4 **********\n",
      "++++++++++ 2 ++++++++++\n",
      "Val loss: 389.297284 | Val tissue acc 0.350000 | Val site acc 0.550000 | Val site auc 0.503759 | Val site f1 0.520000 | Val site auprc 0.606320  \n",
      "Epoch: 1/200 | Train Loss: 1086.022025 | Val loss: 356.358856 | Val tissue acc 0.350000 | Val site acc 0.500000 | Val site auc 0.486216 | Val site f1 0.373041 | Val site auprc 0.603127  \n",
      "Epoch: 51/200 | Train Loss: 64.904295 | Val loss: 128.941005 | Val tissue acc 0.225000 | Val site acc 0.550000 | Val site auc 0.423559 | Val site f1 0.520000 | Val site auprc 0.445575  \n",
      "Epoch: 101/200 | Train Loss: 59.564459 | Val loss: 125.391104 | Val tissue acc 0.275000 | Val site acc 0.575000 | Val site auc 0.481203 | Val site f1 0.481312 | Val site auprc 0.528710  \n",
      "Epoch: 151/200 | Train Loss: 56.923397 | Val loss: 130.710461 | Val tissue acc 0.250000 | Val site acc 0.600000 | Val site auc 0.664160 | Val site f1 0.544160 | Val site auprc 0.639727  \n",
      "############### Best prediction based on average\n",
      "accuracy 0.65\n",
      "f1_score 0.6153846153846154\n",
      "auc 0.6466165413533835\n",
      "auprc 0.6218978023318413\n",
      "++++++++++ 1 ++++++++++\n",
      "Val loss: 328.588251 | Val tissue acc 0.200000 | Val site acc 0.425000 | Val site auc 0.373434 | Val site f1 0.394338 | Val site auprc 0.387060  \n",
      "Epoch: 1/200 | Train Loss: 1079.115179 | Val loss: 417.889362 | Val tissue acc 0.350000 | Val site acc 0.500000 | Val site auc 0.461153 | Val site f1 0.500000 | Val site auprc 0.433947  \n",
      "Epoch: 51/200 | Train Loss: 65.965473 | Val loss: 127.497594 | Val tissue acc 0.275000 | Val site acc 0.350000 | Val site auc 0.388471 | Val site f1 0.348371 | Val site auprc 0.488489  \n",
      "Epoch: 101/200 | Train Loss: 58.204782 | Val loss: 130.092355 | Val tissue acc 0.250000 | Val site acc 0.500000 | Val site auc 0.538847 | Val site f1 0.494949 | Val site auprc 0.511248  \n",
      "Epoch: 151/200 | Train Loss: 57.884408 | Val loss: 125.687294 | Val tissue acc 0.275000 | Val site acc 0.425000 | Val site auc 0.368421 | Val site f1 0.406834 | Val site auprc 0.460396  \n",
      "############### Best prediction based on average\n",
      "accuracy 0.55\n",
      "f1_score 0.5488721804511278\n",
      "auc 0.5789473684210527\n",
      "auprc 0.627777673045966\n",
      "********** 5 **********\n",
      "++++++++++ 2 ++++++++++\n",
      "Val loss: 392.845709 | Val tissue acc 0.225000 | Val site acc 0.525000 | Val site auc 0.548872 | Val site f1 0.344262 | Val site auprc 0.578876  \n",
      "Epoch: 1/200 | Train Loss: 1103.737030 | Val loss: 366.900763 | Val tissue acc 0.350000 | Val site acc 0.500000 | Val site auc 0.473684 | Val site f1 0.500000 | Val site auprc 0.501771  \n",
      "Epoch: 51/200 | Train Loss: 63.007759 | Val loss: 126.911809 | Val tissue acc 0.275000 | Val site acc 0.400000 | Val site auc 0.486216 | Val site f1 0.398496 | Val site auprc 0.543502  \n",
      "Epoch: 101/200 | Train Loss: 56.175979 | Val loss: 125.841832 | Val tissue acc 0.300000 | Val site acc 0.525000 | Val site auc 0.463659 | Val site f1 0.509994 | Val site auprc 0.465527  \n",
      "Epoch: 151/200 | Train Loss: 58.814657 | Val loss: 125.363858 | Val tissue acc 0.325000 | Val site acc 0.525000 | Val site auc 0.393484 | Val site f1 0.447273 | Val site auprc 0.476387  \n",
      "############### Best prediction based on average\n",
      "accuracy 0.55\n",
      "f1_score 0.4871794871794871\n",
      "auc 0.5513784461152882\n",
      "auprc 0.6290381851723985\n",
      "++++++++++ 1 ++++++++++\n",
      "Val loss: 489.455212 | Val tissue acc 0.400000 | Val site acc 0.550000 | Val site auc 0.719298 | Val site f1 0.435737 | Val site auprc 0.675905  \n",
      "Epoch: 1/200 | Train Loss: 1249.387769 | Val loss: 375.646191 | Val tissue acc 0.300000 | Val site acc 0.500000 | Val site auc 0.533835 | Val site f1 0.333333 | Val site auprc 0.510937  \n",
      "Epoch: 51/200 | Train Loss: 63.110736 | Val loss: 128.342743 | Val tissue acc 0.250000 | Val site acc 0.625000 | Val site auc 0.624060 | Val site f1 0.613153 | Val site auprc 0.521145  \n",
      "Epoch: 101/200 | Train Loss: 58.025454 | Val loss: 124.747292 | Val tissue acc 0.325000 | Val site acc 0.500000 | Val site auc 0.619048 | Val site f1 0.450549 | Val site auprc 0.495867  \n",
      "Epoch: 151/200 | Train Loss: 56.356223 | Val loss: 130.330765 | Val tissue acc 0.225000 | Val site acc 0.425000 | Val site auc 0.518797 | Val site f1 0.357093 | Val site auprc 0.445875  \n",
      "############### Best prediction based on average\n",
      "accuracy 0.675\n",
      "f1_score 0.6647324306898774\n",
      "auc 0.5739348370927319\n",
      "auprc 0.5307462881055576\n"
     ]
    }
   ],
   "source": [
    "cv_metrics = dict()\n",
    "\n",
    "# please change this for your own training\n",
    "best_model_dir = '/scratch/gilbreth/wang3712/PreMet_toy_new_data/'\n",
    "\n",
    "if not os.path.exists(best_model_dir):\n",
    "    os.mkdir(best_model_dir)\n",
    "\n",
    "fold = 1\n",
    "\n",
    "for split in cv_splits:\n",
    "\n",
    "    cv_metrics[fold] = dict()\n",
    "    cv_metrics[fold]['accuracy'] = dict()\n",
    "    cv_metrics[fold]['f1_score'] = dict()\n",
    "    cv_metrics[fold]['auc'] = dict()\n",
    "    cv_metrics[fold]['auprc'] = dict()\n",
    "\n",
    "\n",
    "    print('*'*10, fold, '*'*10)\n",
    "\n",
    "    for tissue in all_sites:\n",
    "        print('+'*10, tissue,  '+'*10)\n",
    "        best_model_state_dict = None\n",
    "\n",
    "        best_acc = 0.0\n",
    "        best_site_acc = 0.0\n",
    "        best_tissue_acc = 0.0\n",
    "        best_site_auprc = 0.0\n",
    "        best_site_f1 = 0.0\n",
    "        best_site_auroc = 0.0\n",
    "\n",
    "        best_average_metric = 0.0\n",
    "\n",
    "        all_data = GeneExpressionDataset(torch.tensor(df_normalized.to_numpy(), dtype=torch.float32), \n",
    "                                         torch.tensor(one_hot_labels_tissue, dtype=torch.float32),\n",
    "                                         torch.tensor(one_hot_labels_site_one_vs_all[tissue], dtype=torch.float32))\n",
    "\n",
    "        # Extract the training and validation data based on the indices\n",
    "        train_data = torch.utils.data.Subset(all_data, split['train_idx'])\n",
    "        val_data = torch.utils.data.Subset(all_data, split['val_idx'])\n",
    "\n",
    "        train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "        val_dataloader = DataLoader(val_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        # Create autoencoder instance with specified hidden size\n",
    "        premet_model = PreMet(gene_num=adata.n_vars, latent_size=32, \n",
    "                              num_tumors=len(adata.obs['tumor'].unique()),\n",
    "                              num_metas_sites=len(adata.obs[tissue+'.1va'].unique()),\n",
    "                              hidden_size=64)\n",
    "\n",
    "        # optimizer\n",
    "        optimizer = torch.optim.Adam(premet_model.parameters(), lr = 1e-3)\n",
    "\n",
    "        # Training loop\n",
    "        train_losses = []\n",
    "        validation_losses = []\n",
    "\n",
    "        tissue_accuracies = []\n",
    "\n",
    "        # all these are for metastasis sites\n",
    "        accuracies = []\n",
    "        auc_scores = []\n",
    "        f1_scores = []\n",
    "        auprc_scores = []\n",
    "\n",
    "        num_epochs = 200\n",
    "\n",
    "        # weight for tissue type classification loss\n",
    "        beta = 100.0\n",
    "        # weight for recon loss\n",
    "        alpha = 1.0\n",
    "        # weight for kl div\n",
    "        phi = 10.0\n",
    "        # weight for metas site prediction loss\n",
    "        delta = 0.0\n",
    "\n",
    "        # save best model\n",
    "        best_val_loss = float('inf')\n",
    "\n",
    "        # pre-training stats\n",
    "        premet_model.eval()\n",
    "        with torch.no_grad():  \n",
    "            # for compute accuracy\n",
    "            pred_tissue_probs = []  \n",
    "            tissue_labels = [] \n",
    "\n",
    "            pred_site_probs = []  \n",
    "            site_labels = [] \n",
    "\n",
    "            val_loss = 0.0\n",
    "            val_tissue_loss = 0.0\n",
    "            val_site_loss = 0.0\n",
    "            for data, tissues, sites in val_dataloader:\n",
    "                reconstructed_x, mean, logvar, pred_tissue_labels, pred_site_labels = premet_model(data)\n",
    "                loss, tissue_loss, site_loss = loss_function_premet(data, reconstructed_x, mean, logvar, \n",
    "                                                                    tissue_labels=tissues, pred_tissue_labels=pred_tissue_labels, \n",
    "                                                                    site_labels=sites, pred_site_labels=pred_site_labels,\n",
    "                                                                    beta=beta, alpha=alpha, phi=phi, delta=delta)\n",
    "\n",
    "                val_loss += loss.item()\n",
    "                val_tissue_loss += tissue_loss.item()\n",
    "                val_site_loss += site_loss.item()\n",
    "\n",
    "                pred_tissue_probs.extend(pred_tissue_labels.detach().cpu().numpy())\n",
    "                tissue_labels.extend(tissues.detach().cpu().numpy())\n",
    "\n",
    "                pred_site_probs.extend(pred_site_labels.detach().cpu().numpy())\n",
    "                site_labels.extend(sites.detach().cpu().numpy())\n",
    "                # val_recon_loss += recon_loss.item()\n",
    "\n",
    "            val_loss/= len(val_dataloader)\n",
    "            tissue_acc = accuracy_score(np.argmax(np.array(tissue_labels), axis=1), np.argmax(np.array(pred_tissue_probs), axis=1))\n",
    "\n",
    "            # Calculate metrics for site predictions\n",
    "            site_acc = accuracy_score(np.argmax(np.array(site_labels), axis=1), np.argmax(np.array(pred_site_probs), axis=1))\n",
    "            site_auc_score = roc_auc_score(site_labels, pred_site_probs, multi_class='ovr')\n",
    "            site_f1 = f1(np.argmax(np.array(site_labels), axis=1), np.argmax(np.array(pred_site_probs), axis=1), average='macro')\n",
    "            site_precision, site_recall, _ = prc(np.argmax(np.array(site_labels), axis=1), np.array(pred_site_probs)[:, 1])\n",
    "            site_auprc = auc(site_recall, site_precision)\n",
    "\n",
    "            print(f\"Val loss: {val_loss:.6f} | Val tissue acc {tissue_acc:.6f} | Val site acc {site_acc:.6f} | Val site auc {site_auc_score:.6f} | Val site f1 {site_f1:.6f} | Val site auprc {site_auprc:.6f}  \")\n",
    "\n",
    "            tissue_accuracies.append(tissue_acc)\n",
    "\n",
    "            accuracies.append(site_acc)\n",
    "            auc_scores.append(site_auc_score)\n",
    "            f1_scores.append(site_f1)\n",
    "            auprc_scores.append(site_auprc)\n",
    "\n",
    "        # start training\n",
    "        for epoch in range(num_epochs):\n",
    "            # Initialize  loss\n",
    "            train_loss = 0.0\n",
    "            val_recon_loss = 0.0\n",
    "            val_loss = 0.0\n",
    "\n",
    "            reconst_loss_val = 0.0\n",
    "            classification_loss_val = 0.0\n",
    "\n",
    "            # train\n",
    "            # Train\n",
    "            premet_model.train()\n",
    "            for data, tissues, sites in train_dataloader:\n",
    "                reconstructed_x, mean, logvar, pred_tissue_labels, pred_site_labels = premet_model(data)\n",
    "                loss, tissue_loss, site_loss = loss_function_premet(data, reconstructed_x, mean, logvar, \n",
    "                                                                    tissue_labels=tissues, pred_tissue_labels=pred_tissue_labels, \n",
    "                                                                    site_labels=sites, pred_site_labels=pred_site_labels,\n",
    "                                                                    beta=beta, alpha=alpha, phi=phi, delta=delta)\n",
    "\n",
    "                # Backward pass and optimization\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # Accumulate loss for the epoch\n",
    "                train_loss += loss.item()  \n",
    "\n",
    "\n",
    "            # Validation \n",
    "            premet_model.eval()\n",
    "            val_tissue_loss = 0.0\n",
    "            val_loss = 0.0\n",
    "            with torch.no_grad():  \n",
    "                # for compute accuracy\n",
    "                pred_tissue_probs = []  \n",
    "                tissue_labels = [] \n",
    "\n",
    "                pred_site_probs = []  \n",
    "                site_labels = [] \n",
    "\n",
    "                val_loss = 0.0\n",
    "                val_tissue_loss = 0.0\n",
    "                val_site_loss = 0.0\n",
    "\n",
    "                for data, tissues, sites in val_dataloader:\n",
    "                    reconstructed_x, mean, logvar, pred_tissue_labels, pred_site_labels = premet_model(data)\n",
    "                    loss, tissue_loss, site_loss = loss_function_premet(data, reconstructed_x, mean, logvar, \n",
    "                                                                        tissue_labels=tissues, pred_tissue_labels=pred_tissue_labels, \n",
    "                                                                        site_labels=sites, pred_site_labels=pred_site_labels,\n",
    "                                                                        beta=beta, alpha=alpha, phi=phi, delta=delta)\n",
    "\n",
    "                    val_loss += loss.item()\n",
    "                    val_tissue_loss += tissue_loss.item()\n",
    "                    val_site_loss += site_loss.item()\n",
    "\n",
    "                    pred_tissue_probs.extend(pred_tissue_labels.detach().cpu().numpy())\n",
    "                    tissue_labels.extend(tissues.detach().cpu().numpy())\n",
    "\n",
    "                    pred_site_probs.extend(pred_site_labels.detach().cpu().numpy())\n",
    "                    site_labels.extend(sites.detach().cpu().numpy())\n",
    "\n",
    "\n",
    "            # compute accuracy\n",
    "            # Print epoch information\n",
    "            tissue_acc = accuracy_score(np.argmax(np.array(tissue_labels), axis=1), np.argmax(np.array(pred_tissue_probs), axis=1))\n",
    "\n",
    "            tissue_accuracies.append(tissue_acc)\n",
    "\n",
    "            # Calculate metrics for site predictions\n",
    "            site_acc = accuracy_score(np.argmax(np.array(site_labels), axis=1), np.argmax(np.array(pred_site_probs), axis=1))\n",
    "            site_auc_score = roc_auc_score(site_labels, pred_site_probs, multi_class='ovr')\n",
    "            site_f1 = f1(np.argmax(np.array(site_labels), axis=1), np.argmax(np.array(pred_site_probs), axis=1), average='macro')\n",
    "            site_precision, site_recall, _ = prc(np.argmax(np.array(site_labels), axis=1), np.array(pred_site_probs)[:, 1])\n",
    "            site_auprc = auc(site_recall, site_precision)\n",
    "\n",
    "            accuracies.append(site_acc)\n",
    "            auc_scores.append(site_auc_score)\n",
    "            f1_scores.append(site_f1)\n",
    "            auprc_scores.append(site_auprc)\n",
    "\n",
    "\n",
    "            # Average losses\n",
    "            train_loss /= len(train_dataloader)\n",
    "            val_loss /= len(val_dataloader)\n",
    "            val_tissue_loss /= len(val_dataloader)\n",
    "\n",
    "            # Save the best model\n",
    "            if site_acc+site_auc_score+site_f1+site_auprc > best_average_metric:\n",
    "                best_average_metric = site_acc+site_auc_score+site_f1+site_auprc\n",
    "                torch.save(premet_model.state_dict(), best_model_dir+tissue+'.split_'+str(fold)+'.pth')\n",
    "\n",
    "            # if the tissue type acc is really good, start to decrease the weight\n",
    "            if tissue_acc >= 0.6:\n",
    "                beta -= 2\n",
    "                beta = max(0, beta)\n",
    "                delta += 5\n",
    "\n",
    "            if tissue_acc <= 0.6 and tissue_acc>= 0.5:\n",
    "                beta += 2\n",
    "\n",
    "            if epoch % 50 ==0:\n",
    "                print(f\"Epoch: {epoch+1}/{num_epochs} | Train Loss: {train_loss:.6f} | Val loss: {val_loss:.6f} | Val tissue acc {tissue_acc:.6f} | Val site acc {site_acc:.6f} | Val site auc {site_auc_score:.6f} | Val site f1 {site_f1:.6f} | Val site auprc {site_auprc:.6f}  \")\n",
    "                # print(f\"Epoch: {epoch+1}/{num_epochs} | Train Loss: {train_loss:.6f} | Val Loss: {val_loss:.6f} | Val tissue acc {tissue_acc:.6f} | Val tissue loss: {val_tissue_loss:.6f} | Val site acc {site_acc:.6f} | Val site loss: {val_site_loss:.6f} \")\n",
    "            train_losses.append(train_loss)\n",
    "            validation_losses.append(val_loss)\n",
    "\n",
    "            # reduce the beta value\n",
    "            # beta -= 5\n",
    "            phi -= 1\n",
    "            phi = max(0, phi)\n",
    "\n",
    "\n",
    "        cv_metrics[fold]['accuracy'][tissue] = accuracies\n",
    "        cv_metrics[fold]['f1_score'][tissue] = f1_scores\n",
    "        cv_metrics[fold]['auc'][tissue] = auc_scores\n",
    "        cv_metrics[fold]['auprc'][tissue] = auprc_scores\n",
    "\n",
    "        # find the max average\n",
    "        avg_metrics = np.array([accuracies, f1_scores, auc_scores, auprc_scores]).sum(axis=0)/4\n",
    "        max_avg_metric_epoch = np.argmax(avg_metrics) + 1\n",
    "\n",
    "        print('#'*15, 'Best prediction based on average')   \n",
    "        for metric in cv_metrics[fold].keys():\n",
    "            print(metric, cv_metrics[fold][metric][tissue][max_avg_metric_epoch-1])\n",
    "\n",
    "    fold +=1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a9044f-cbe6-4c9a-a15f-0c12a73c8c87",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "220070c8-39c2-41dd-9d34-07677cfe939b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def premet_predict(model, data_loader):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for data in data_loader:\n",
    "            # print(data)\n",
    "            data = data[0].to(next(model.parameters()).device)  # Ensure data is on the same device as the model\n",
    "            mean, logvar = model.encoder(data)\n",
    "            z = model.reparameterize(mean, logvar)\n",
    "            labels = model.site_nn(z)\n",
    "            # print(labels)\n",
    "            all_labels.append(labels)\n",
    "            \n",
    "    all_labels = torch.cat(all_labels, dim=0).cpu().numpy()\n",
    "    return all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "816dc059-d316-47b3-ae15-0a47fbecd183",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_best_models = os.listdir(best_model_dir)\n",
    "all_best_models.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ee40c0be-65b1-4cf1-a7bd-78bb61c7633c",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_prediction_metric_dict = dict()\n",
    "prior_prediction_metric_dict = dict()\n",
    "\n",
    "all_ground_truth_dfs = dict()\n",
    "all_prediction_value_dfs = dict()\n",
    "all_metrics = ['avg']\n",
    "for metric_idx in range(len(all_metrics)):\n",
    "    final_prediction_metric_dict[all_metrics[metric_idx]] = dict()\n",
    "    prior_prediction_metric_dict[all_metrics[metric_idx]] = dict()\n",
    "    all_ground_truth_dfs[all_metrics[metric_idx]] = dict()\n",
    "    all_prediction_value_dfs[all_metrics[metric_idx]] = dict()\n",
    "    \n",
    "    fold = 1\n",
    "    prior_accuracies = []\n",
    "    prior_f1_scores = []\n",
    "    prior_aurocs = []\n",
    "    prior_auprcs = []\n",
    "\n",
    "    accuracies = []\n",
    "    f1_scores = []\n",
    "    aurocs = []\n",
    "    auprcs = []\n",
    "\n",
    "    for split in cv_splits:\n",
    "        # print('*' * 10, fold, '*' * 10)\n",
    "        \n",
    "        all_ground_truth_dfs[all_metrics[metric_idx]][fold] = dict()\n",
    "        all_prediction_value_dfs[all_metrics[metric_idx]][fold] = dict()\n",
    "\n",
    "        all_data = GeneExpressionDataset(torch.tensor(df_normalized.to_numpy(), dtype=torch.float32), \n",
    "                                        torch.tensor(one_hot_labels_tissue, dtype=torch.float32),\n",
    "                                        torch.tensor(one_hot_labels_site, dtype=torch.float32))\n",
    "        # Extract the training and validation data based on the indices\n",
    "        val_data = torch.utils.data.Subset(all_data, split['val_idx'])\n",
    "        val_dataloader = DataLoader(val_data, batch_size=batch_size, shuffle=False)        \n",
    "\n",
    "        # load the best model for different metrics\n",
    "\n",
    "        for metric in [all_metrics[metric_idx]]:\n",
    "            # print('+' * 10, metric, '+' * 10)\n",
    "            best_models = dict()\n",
    "            # load the model\n",
    "            for site in all_sites:\n",
    "                # print('$' * 10, site, '$' * 10)\n",
    "                model = PreMet(gene_num=adata.n_vars, latent_size=32, \n",
    "                                num_tumors=len(adata.obs['tumor'].unique()),\n",
    "                                num_metas_sites=len(adata.obs[tissue + '.1va'].unique()),\n",
    "                                hidden_size=64)\n",
    "\n",
    "                model.load_state_dict(torch.load(best_model_dir + site + '.split_' + str(fold) +'.pth'))\n",
    "                best_models[site] = model\n",
    "\n",
    "            predictions = dict()\n",
    "            ground_truth_dfs = adata.obs.iloc[split['val_idx']]\n",
    "            for site in all_sites:\n",
    "                preds = premet_predict(best_models[site], val_dataloader)\n",
    "                # the second column indicates sample predicted to be this label\n",
    "                predictions[site] = preds[:, 1]\n",
    "            prediction_df = pd.DataFrame.from_dict(predictions)\n",
    "            new_names = [s + '.1va' for s in all_sites]\n",
    "            ground_truth_dfs = ground_truth_dfs[new_names]\n",
    "            \n",
    "            # save the predictions and ground truth for confusionmatrix\n",
    "            all_ground_truth_dfs[all_metrics[metric_idx]][fold] = ground_truth_dfs\n",
    "            all_prediction_value_dfs[all_metrics[metric_idx]][fold] = prediction_df\n",
    "\n",
    "            # Convert the DataFrames to numpy arrays\n",
    "            true_labels = ground_truth_dfs.to_numpy()\n",
    "            pred_probs = prediction_df.to_numpy()\n",
    "\n",
    "            # Calculate priors\n",
    "\n",
    "            # Convert one-hot encoded labels to class indices\n",
    "            true_labels_val = np.argmax(true_labels, axis=1)\n",
    "\n",
    "            # Calculate the class distribution\n",
    "            class_counts = np.bincount(true_labels_val)\n",
    "            total_samples = len(true_labels_val)\n",
    "            most_frequent_class = class_counts.max()\n",
    "\n",
    "            # Prior Accuracy: The frequency of the most common class\n",
    "            prior_accuracy = most_frequent_class / total_samples\n",
    "\n",
    "            # Prior F1 Score: Macro F1 score assuming all samples are predicted as the most frequent class\n",
    "            dummy_predictions = np.full_like(true_labels_val, fill_value=np.argmax(class_counts))  # Predicting the most frequent class for all\n",
    "            prior_f1_score = f1(true_labels_val, dummy_predictions, average='macro')\n",
    "\n",
    "            # Prior AUROC: This will be 0.5 for random guessing in multi-class problems (you can calculate class-wise AUROC if needed)\n",
    "            prior_auroc = 0.5\n",
    "\n",
    "            # Prior AUPRC: Class-wise AUPRC (One-vs-Rest)\n",
    "            prior_auprc = average_precision_score(np.eye(len(class_counts))[true_labels_val], np.eye(len(class_counts))[dummy_predictions], average='macro')\n",
    "\n",
    "            # Store prior metrics\n",
    "            prior_accuracies.append(prior_accuracy)\n",
    "            prior_f1_scores.append(prior_f1_score)\n",
    "            prior_aurocs.append(prior_auroc)\n",
    "            prior_auprcs.append(prior_auprc)\n",
    "\n",
    "            # Print prior metrics\n",
    "            \"\"\"\n",
    "            print(f'Prior Accuracy: {prior_accuracy:.4f}')\n",
    "            print(f'Prior F1 Score: {prior_f1_score:.4f}')\n",
    "            print(f'Prior AUROC: {prior_auroc:.4f}')\n",
    "            print(f'Prior AUPRC: {prior_auprc:.4f}')\n",
    "            print('#' * 10)\n",
    "            \"\"\"\n",
    "\n",
    "            # Calculate metrics\n",
    "            accuracy = accuracy_score(np.argmax(true_labels, axis=1), np.argmax(pred_probs, axis=1))\n",
    "            f1_score = f1(np.argmax(true_labels, axis=1), np.argmax(pred_probs, axis=1), average='macro')\n",
    "            auroc = roc_auc_score(true_labels, pred_probs, multi_class='ovr')\n",
    "\n",
    "            # Calculate AUPRC for each class and average\n",
    "            precision = dict()\n",
    "            recall = dict()\n",
    "            auprc = []\n",
    "            for i in range(true_labels.shape[1]):\n",
    "                precision[i], recall[i], _ = prc(true_labels[:, i], pred_probs[:, i])\n",
    "                auprc.append(auc(recall[i], precision[i]))\n",
    "            avg_auprc = np.mean(auprc)\n",
    "\n",
    "            # Store metrics\n",
    "            accuracies.append(accuracy)\n",
    "            f1_scores.append(f1_score)\n",
    "            aurocs.append(auroc)\n",
    "            auprcs.append(avg_auprc)\n",
    "\n",
    "        fold += 1\n",
    "\n",
    "    prior_prediction_metric_dict[all_metrics[metric_idx]]['f1'] = prior_f1_scores\n",
    "    prior_prediction_metric_dict[all_metrics[metric_idx]]['accuracy'] = prior_accuracies\n",
    "    prior_prediction_metric_dict[all_metrics[metric_idx]]['auroc'] = prior_aurocs\n",
    "    prior_prediction_metric_dict[all_metrics[metric_idx]]['auprc'] = prior_auprcs\n",
    "        \n",
    "    final_prediction_metric_dict[all_metrics[metric_idx]]['f1'] = f1_scores\n",
    "    final_prediction_metric_dict[all_metrics[metric_idx]]['accuracy'] = accuracies\n",
    "    final_prediction_metric_dict[all_metrics[metric_idx]]['auroc'] = aurocs\n",
    "    final_prediction_metric_dict[all_metrics[metric_idx]]['auprc'] = auprcs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "21e459f5-b575-47cf-81d8-0093757eb348",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>auroc</th>\n",
       "      <th>auprc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.619048</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.640351</td>\n",
       "      <td>0.648662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.649123</td>\n",
       "      <td>0.650</td>\n",
       "      <td>0.669173</td>\n",
       "      <td>0.690213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.544160</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.739348</td>\n",
       "      <td>0.715302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.670330</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.612782</td>\n",
       "      <td>0.624838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.404762</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.562657</td>\n",
       "      <td>0.579892</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         f1  accuracy     auroc     auprc\n",
       "0  0.619048     0.625  0.640351  0.648662\n",
       "1  0.649123     0.650  0.669173  0.690213\n",
       "2  0.544160     0.600  0.739348  0.715302\n",
       "3  0.670330     0.700  0.612782  0.624838\n",
       "4  0.404762     0.500  0.562657  0.579892"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# print training results from best model across different folds\n",
    "best_metric = 'avg'\n",
    "best_final_prediction_metrid_df = pd.DataFrame.from_dict(final_prediction_metric_dict[best_metric])\n",
    "display(best_final_prediction_metrid_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adfc5e02-2166-4493-8440-d3eb342e611d",
   "metadata": {},
   "source": [
    "# Predict metastasis potential for new samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8a9b2ff9-f166-4b31-8c37-d7f873e9972f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.35844174, -1.01771296, -1.35433862, ..., -1.13175601,\n",
       "         0.        ,  0.05709045],\n",
       "       [ 0.41962336,  2.00519682, -0.46332637, ...,  1.82065098,\n",
       "         0.47036043, -1.37017083],\n",
       "       [-0.6472157 , -1.01771296, -1.35433862, ..., -0.63968818,\n",
       "         0.94072087, -0.65654019],\n",
       "       ...,\n",
       "       [ 1.48646242,  1.50137853, -0.90883249, ...,  0.83651531,\n",
       "         0.        ,  2.91161301],\n",
       "       [-1.00282872, -0.01007637,  0.42768588, ..., -0.63968818,\n",
       "        -1.4110813 , -1.01335551],\n",
       "       [-1.00282872,  0.49374193, -1.35433862, ..., -0.63968818,\n",
       "         0.94072087,  0.77072109]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# start from some toy data\n",
    "\n",
    "num_samples = 50\n",
    "\n",
    "# Generate cluster centers (mean expression levels for each cluster)\n",
    "cluster_centers = np.random.normal(loc=gene_expression_min, scale=cluster_spread, size=(num_clusters, num_genes))\n",
    "\n",
    "# Generate labels for each sample (which cluster they belong to)\n",
    "sample_labels = np.random.choice(num_clusters, num_samples)\n",
    "\n",
    "# Create an empty matrix to hold the RNA-seq data\n",
    "simulated_data = np.zeros((num_samples, num_genes))\n",
    "\n",
    "# Generate RNA-seq like data with noise\n",
    "for i in range(num_samples):\n",
    "    # Get the cluster center for this sample\n",
    "    cluster_center = cluster_centers[sample_labels[i], :]\n",
    "    \n",
    "    # Add noise to simulate RNA-seq counts\n",
    "    simulated_data[i, :] = np.random.negative_binomial(n=10, p=np.clip(1 - cluster_center / (cluster_center + 10), 0.01, 0.99))\n",
    "\n",
    "# Convert to pandas DataFrame for easy manipulation\n",
    "simulated_df = pd.DataFrame(simulated_data, columns=[f'Gene_{i+1}' for i in range(num_genes)], index=[f'Sample_{i+1}' for i in range(num_samples)])\n",
    "simulated_df['Cluster'] = sample_labels\n",
    "\n",
    "scaler = StandardScaler()\n",
    "new_data = scaler.fit_transform(simulated_df.drop('Cluster', axis=1))\n",
    "new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5ab7427f-3a64-4706-88bf-21fd859df897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# min max normalize the data\n",
    "new_data = pd.DataFrame(new_data).apply(lambda x: (x - x.min()) / (x.max() - x.min()), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "502c715a-667a-4ecf-88ba-e567e75e493a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PreMet import PreMet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9123e9cd-33b9-4e5a-9b12-0918449427c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# provide the trained model directory\n",
    "model_dir = '/scratch/gilbreth/wang3712/PreMet_toy_new_data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3b22b9ea-04be-467d-9d11-4e4289a79665",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sites = ['1', '2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d2a55025-a237-4ba3-841b-a3b25f7af71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in Dataloader\n",
    "\n",
    "# put zero for labels, won't be used\n",
    "new_data = GeneExpressionDataset(torch.tensor(new_data.to_numpy(), dtype=torch.float32), \n",
    "                                 torch.tensor([0]*len(new_data.to_numpy()), dtype=torch.float32),\n",
    "                                 torch.tensor([0]*len(new_data.to_numpy()), dtype=torch.float32))\n",
    "new_data_dataloader = DataLoader(new_data, batch_size=8, shuffle=False)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f34f3f5d-d4cf-4d2f-8112-95cedb019cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = dict()\n",
    "# specify a fold\n",
    "fold = 1\n",
    "for site in all_sites:\n",
    "    model = PreMet(gene_num=num_genes, latent_size=32, \n",
    "                        num_tumors=num_tumor_type,\n",
    "                        num_metas_sites=num_mets_site,\n",
    "                        hidden_size=64)\n",
    "\n",
    "    model.load_state_dict(torch.load(model_dir + site + '.split_' + str(fold) +'.pth'))\n",
    "    # best_models[site] = model\n",
    "    preds = premet_predict(model, new_data_dataloader)\n",
    "    predictions[site] = preds[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e14ad8ed-56d8-4ac1-a605-f7359a7a6980",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Neg</th>\n",
       "      <th>Brain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.479526</td>\n",
       "      <td>0.500781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.561121</td>\n",
       "      <td>0.540694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.566997</td>\n",
       "      <td>0.546126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.523460</td>\n",
       "      <td>0.366221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.476122</td>\n",
       "      <td>0.575463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.490564</td>\n",
       "      <td>0.454563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.510441</td>\n",
       "      <td>0.619042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.484028</td>\n",
       "      <td>0.528666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.529696</td>\n",
       "      <td>0.692056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.493476</td>\n",
       "      <td>0.469105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.555737</td>\n",
       "      <td>0.488076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.513106</td>\n",
       "      <td>0.606893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.542405</td>\n",
       "      <td>0.445687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.463080</td>\n",
       "      <td>0.438264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.564381</td>\n",
       "      <td>0.508566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.493356</td>\n",
       "      <td>0.478688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.595416</td>\n",
       "      <td>0.526694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.522263</td>\n",
       "      <td>0.485513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.560507</td>\n",
       "      <td>0.550416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.515995</td>\n",
       "      <td>0.514803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.539288</td>\n",
       "      <td>0.554639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.562043</td>\n",
       "      <td>0.361914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.444810</td>\n",
       "      <td>0.563865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.585036</td>\n",
       "      <td>0.538186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.501105</td>\n",
       "      <td>0.500096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.476321</td>\n",
       "      <td>0.454656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.540915</td>\n",
       "      <td>0.555484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.482690</td>\n",
       "      <td>0.583417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.490566</td>\n",
       "      <td>0.461920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.540832</td>\n",
       "      <td>0.402064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.532404</td>\n",
       "      <td>0.520737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.565657</td>\n",
       "      <td>0.632264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.513151</td>\n",
       "      <td>0.555679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.535775</td>\n",
       "      <td>0.567516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.511880</td>\n",
       "      <td>0.596165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.442450</td>\n",
       "      <td>0.627136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.578392</td>\n",
       "      <td>0.374041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.467254</td>\n",
       "      <td>0.479082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.505157</td>\n",
       "      <td>0.541157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.615426</td>\n",
       "      <td>0.557792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.569231</td>\n",
       "      <td>0.456780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.545844</td>\n",
       "      <td>0.531985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.443774</td>\n",
       "      <td>0.437195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.573449</td>\n",
       "      <td>0.598923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.536426</td>\n",
       "      <td>0.579728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.483371</td>\n",
       "      <td>0.497861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.499715</td>\n",
       "      <td>0.498237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.506035</td>\n",
       "      <td>0.508745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.484514</td>\n",
       "      <td>0.521207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.540678</td>\n",
       "      <td>0.506745</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Neg     Brain\n",
       "0   0.479526  0.500781\n",
       "1   0.561121  0.540694\n",
       "2   0.566997  0.546126\n",
       "3   0.523460  0.366221\n",
       "4   0.476122  0.575463\n",
       "5   0.490564  0.454563\n",
       "6   0.510441  0.619042\n",
       "7   0.484028  0.528666\n",
       "8   0.529696  0.692056\n",
       "9   0.493476  0.469105\n",
       "10  0.555737  0.488076\n",
       "11  0.513106  0.606893\n",
       "12  0.542405  0.445687\n",
       "13  0.463080  0.438264\n",
       "14  0.564381  0.508566\n",
       "15  0.493356  0.478688\n",
       "16  0.595416  0.526694\n",
       "17  0.522263  0.485513\n",
       "18  0.560507  0.550416\n",
       "19  0.515995  0.514803\n",
       "20  0.539288  0.554639\n",
       "21  0.562043  0.361914\n",
       "22  0.444810  0.563865\n",
       "23  0.585036  0.538186\n",
       "24  0.501105  0.500096\n",
       "25  0.476321  0.454656\n",
       "26  0.540915  0.555484\n",
       "27  0.482690  0.583417\n",
       "28  0.490566  0.461920\n",
       "29  0.540832  0.402064\n",
       "30  0.532404  0.520737\n",
       "31  0.565657  0.632264\n",
       "32  0.513151  0.555679\n",
       "33  0.535775  0.567516\n",
       "34  0.511880  0.596165\n",
       "35  0.442450  0.627136\n",
       "36  0.578392  0.374041\n",
       "37  0.467254  0.479082\n",
       "38  0.505157  0.541157\n",
       "39  0.615426  0.557792\n",
       "40  0.569231  0.456780\n",
       "41  0.545844  0.531985\n",
       "42  0.443774  0.437195\n",
       "43  0.573449  0.598923\n",
       "44  0.536426  0.579728\n",
       "45  0.483371  0.497861\n",
       "46  0.499715  0.498237\n",
       "47  0.506035  0.508745\n",
       "48  0.484514  0.521207\n",
       "49  0.540678  0.506745"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# each col is a metastasis site label\n",
    "# assume we have two labels\n",
    "predictions = pd.DataFrame.from_dict(predictions)\n",
    "predictions.columns = ['Neg', 'Brain']\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83725785-8673-4602-9508-5ba3a5c606be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "premet",
   "language": "python",
   "name": "premet"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
